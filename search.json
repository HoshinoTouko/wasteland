[{"title":"Hello World","url":"//blog/2020/03/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/hello-world/ Written with Passion and Hope\n"},{"title":"Start with Hexo","url":"//blog/2020/03/start-with-hexo/","content":"一切的结束，一切的开始我又搬博客了。在咕了两年零两个月之后，我终于继续找地方写我想写的东西了。此时应该放个烟花庆祝一下。如果你的城市不能放烟花，请打开一首打上花火。  \n我把以前的博客封存了起来。在搬家时，我顺便翻了翻它。我发现每次搬家，都会再次发觉曾经的我的幼稚。当然人是要磨练的，在不断否定自己、改变自己后，才能在成长的路上艰难地向前挪动一步。\n但我也会惊讶，惊讶于曾经渺小的自己爆发出来的强大的能量，和童言无忌口无遮拦的那些存粹的话。要是让我现在去修改以前的文章，可能要大段大段的砍掉让我现在念起来会面红耳赤的内容。我最终决定还是将他们悉数保留，也算是对我的青春的一种纪念。\n鞭尸有的文章实在值得我自己翻出来狠狠的抽自己不算厚脸皮，感受这种尴尬气氛下微妙的喉咙干燥感。以下文章全部摘自旧博客 日常 分类，并亲自附上吐槽。\n\n我与开发的故事我没想到当时的我竟有勇气将 Link 的内容设置为 my-way-to-developer，因为现在的我越发没有自信称自己为开发者。我觉得我自己就是个写代码造软件赚点外快的。可能这就是小孩子的勇气吧。\n我和typecho的第一次(一篇中规中矩的初级技术文章，伴随着一个自言自语式的尴尬的开场。当时的我竟然可以突破重重看不懂的代码，找到问题的核心所在，还能依葫芦画瓢写一些 “it works” 的 Apache 配置文件，实在难得。这样一个聪明的小孩，竟然没有在 SE 的道路上继续走下去，实在是可惜了。\n再見，我的競賽生涯就这点数学物理知识还是不要说自己是打竞赛的好。我感觉以前学竞赛学的那点皮毛，现在我把自己按在椅子前学两天，也能把题目做的头头是道。哇，竟然还爱上杭州，半年前我提着行李箱顶着烈日在没地铁的浙大西溪校区边上走着的时候肯定无比后悔曾经说过的话。不过，只身一人从杭州前往深圳、中途中转广州的经历算是很深的记忆了。虽然他们可能远不及现在的日子精彩，但作为那时的我，也是一个值得纪念的挑战。\n\n关于旧博客的存档旧博客我还是做好了存档，网址是 https://savepoint.touko.moe/ 。也不知道当时是为何停止维护了，明明刚开了两个项目连载，分别是 MIPS CPU 和 bot-framework，结果最后也没能继续做下去。\n在考虑旧博客命名时，我和 POJO 讨论了如下几个选项。\n\narchive\nlegacy\ndeprecated\nmilestone\npyramid\nsavepoint\nruin\n\n这下可好，已经把未来数次的博客搬迁取名事宜考虑好了，以后可以尽情搬迁新博客而不用担心二级域不够用。\n\n\n关于新博客取名至于为啥叫 wasteland，大概只是因为它很帅。当我在考虑新博客取名时，随口问了句 POJO 我该用啥名好。\nPOJO: “teenage wasteland” 。\n虽然我没了解过相关的歌，但是觉得这个名字实在有意思。鉴于我已经到了奔三的年龄，teenage 还是不必了，那么就取名为 wasteland 好了。我会把我想说的，有用的没用的，技术的非技术的，碎碎念还是吐槽啥的，一股脑丢到这。如果几年之后，这里的文章有人能够全读下来，那说不定对我的了解比我自己还深。\n\n\n是的，谷歌翻译很酷\n技术相关Savepoint - 迁移旧博客让 Typecho 运行在 PHP 7旧博客我用的是运行于 PHP 的 Typecho ，甚至一度还是 0.9 版本，三年前的某天终于找了机会升级到了 1.0。我把整站数据库打包，PHP文件打包，装在了新的网站上。旧网站使用的是 PHP 5.6 + Apache 2.4 + MySQL 5.6 的组合，新网站已经全站切换至 PHP 7.3 + NginX 1.17 + MySQL 5.6 的组合。目前看来，NginX 用起来更顺手，并且资源消耗相比前一套系统要少一些。  \n意料之内，在部署过程中遇上了数据库问题。更新 MySQL 账号密码后，页面仍然提示 Database Server Error，查看访问日志发现报错为 PHP message: Adapter Typecho_Db_Adapter_Mysql is not available。经排查，发现是由于 Typecho 默认使用的 MySQL 适配器 Mysql 在 PHP 7 环境下已经不再使用，只需要将 config.inc.php 文件进行如下修改，改为使用 Pdo_Mysql 进行数据库连接操作。\n// $db = new Typecho_db(&quot;Mysql&quot;,&quot;typecho_&quot;);$db = new Typecho_db(&quot;Pdo_Mysql&quot;,&quot;typecho_&quot;);\n\n还要对 Typecho 的伪静态进行迁移。由于前期我使用的是 Apache，现在使用 NginX，配置文件工作方式有一定变化。\n&lt;IfModule mod_rewrite.c&gt;RewriteEngine OnRewriteBase /RewriteCond %&#123;REQUEST_FILENAME&#125; !-dRewriteCond %&#123;REQUEST_FILENAME&#125; !-fRewriteRule ^(.*)$ /index.php/$1 [L]RewriteCond %&#123;HTTP_HOST&#125; ^www.touko.moe RewriteRule (.*) http://touko.moe/$1 [R=301,L]RewriteCond %&#123;HTTPS&#125; !^on$ RewriteRule (.*) https://%&#123;SERVER_NAME&#125;%&#123;REQUEST_URI&#125; [R=301] &lt;/IfModule&gt;\n\n上面是我曾经写下的 Apache 配置文件，主要做了三件事\n\n将找不到的文件路径隐性转发给 index.php 处理请求 （伪静态）\n将 www.touko.moe 301 重定向至 touko.moe 处理\n将非 HTTPS 请求 301 重定向至 HTTPS（强制 HTTPS）\n\n在 NginX 中，上述设置修改为如下形式\nif (!-e $request_filename) &#123;    rewrite ^(.*)$ /index.php$1 last;&#125;if ($server_port !~ 443)&#123;    rewrite ^(/.*)$ https://$host$1 permanent;&#125;\n\nSEO 相关为了让曾经的链接可以正常访问，我做了一个小处理，让 NginX 妥善处理旧请求至旧博客存档点。由于只是域名发生了变化，只需要针对特定 URL Pattern 做 301 重定向即可。我检查了一下旧网站上文章出现的 URL ，找到了几个关键词，用以下规则完成转发\nrewrite ^/(category|blog|author|links|aboutme|archives|usr)(.*) http://savepoint.touko.moe/$1$2 permanent;\n\n举个例子，当访问页面 https://touko.moe/blog/MonocycleCPU_MIPS 时，NginX 将匹配 blog 和 /MonocycleCPU_MIPS 作为参数 1 和参数 2，并 301 重定向至 http://savepoint.touko.moe/&#123;1&#125;&#123;2&#125; ，也就是 &#96;&#96;http://savepoint.touko.moe/{blog}{/MonocycleCPU_MIPS}` 这样就起到正常访问原网页的效果，并且对搜索引擎的影响也有一定的降低。\n同时，因为我使用了 Disqus 评论系统，还需要在他们网站上进行站点转移。在 Disqus 设置页面的 Configure Disqus for Your Site 下，可以找到 Changing domains? Learn how. 在这里就可以设置一键转移了。\nGithub Pages 反向代理我偷懒使用 Jekyll 建立了我的 Under Construction 页面。因为需要转发旧博客的访问，希望使用我的 NginX 处理所有来自 https://touko.moe/ 的请求，而不是 CNAME。在设置 Github Pages 反向代理时，需要使用假的 header 欺骗 Github 服务器，避免 404 和被多次重定向。\nlocation / &#123;    proxy_pass https://hoshinotouko.github.io;    proxy_redirect     off;    proxy_set_header Host hoshinotouko.github.io;    proxy_set_header X-Real-IP $remote_addr;    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    proxy_set_header REMOTE-HOST $remote_addr;&#125;\n\n其中，这句配置非常重要。\nproxy_set_header Host hoshinotouko.github.io; \n这样一来，网站就能呈现如下组织方式。\ntouko.moe - Under construction (Will be a navi page in the future)  |  |- touko.moe/post/xxx -&gt; savepoint.touko.moe/post/xxx  |  |- savepoint.touko.moe  |  |- wasteland.touko.moe  |  ...\n\nWasteland - 新博客我使用 Hexo 将网站部署在 Github Pages 上，解析为 https://wasteland.touko.moe/ 。\n不得不说 Hexo 真是个方便的东西，日常使用只需要几个命令就能完成。\n$ hexo clean // Delete all generated files$ hexo g // Generate$ hexo d // Deploy$ hexo s // Local development server\n\n我使用了爆款 NexT 主题，因为这个主题的确简单好看。我随意增加了几个插件，用于改善博客的开发和访问体验\n\nhttps://github.com/acwong00/hexo-addlink\nhttps://github.com/hexojs/hexo-deployer-git\n\nAutomatically Deploy可以通过在 Hexo 增加插件的方式，快速将静态页面部署至 Github Pages 。我使用了 hexo-deployer-git 插件。首先 yarn add hexo-deployer-git ，其次进行配置，在 _config.yml 中添加如下设置字段。\n# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:  type: git  repo: git@github.com:HoshinoTouko/wasteland.git  branch: gh-pages\n\n我把 Github Pages 的页面放在 gh-pages branch 下，master branch 用于存放网站的源代码。所以在上方配置文件中我这样写。但是要注意，使用插件进行更新，会触发 git 的 force push，尽量不要直接在生成的网页文件中进行修改。\nGit SSH Key因为我开了 Github 两步验证，每次部署都要收一下手机验证码，实在有点讨厌，所以我打算直接在本地创建一份 SSH Key 用于部署。我使用的是 Windows，Linux 和 Mac 的配置应该类似。\n$ ssh-keygen -t rsa -C &quot;your_email@mail.com&quot;$ eval $(ssh-agent -s)$ ssh-add /c/Users/&#123;USERNAME&#125;/.ssh/id_rsa$ clip &lt; /c/Users/&#123;USERNAME&#125;/.ssh/id_rsa.pub\n\n然后将剪贴板中的公钥上传至 Github - Settings - SSH Keys ，就可以直接通过 hexo d 部署网页了。\nCNAME of Github Pages上文提到，hexo d 的更新会进行一次 force push，因此在 Github Setting 页面进行 CNAME 的设置会被下一次 force push 覆盖。为了解决这个问题，需要创建文件 source/CNAME 并设置为\nwasteland.touko.moe\n\n之后进行 hexo g &amp;&amp; hexo d 操作时，CNAME 记录就会被自动保存。\nCloudFlare 面板访问问题我将域名解析由 DNSPod 全部切换至 CloudFlare 。设置好 CF 的 HTTPS 访问后，我发现我的服务器面板（非 80 &#x2F; 443 端口）无法访问了。经检查，是由于如下原因\n\n需要注意的是，中国境内的 HTTP&#x2F;HTTPS 流量节点只支持 80 和 443 端口。\n\n倒也不用担心，只需要将其关闭，或者挂梯子访问就可以了。\nReferences\nhttps://www.typechodev.com/case/Adapter-Typecho_Db_Adapter_Mysql-is-not-available.html\nhttps://zhuanlan.zhihu.com/p/26625249\nhttps://www.jianshu.com/p/9317a927e844\nhttps://blog.csdn.net/JOYIST/article/details/90514991\nhttps://www.jianshu.com/p/8b25564d3ff3\nhttps://io-oi.me/tech/hexo-next-optimization/#%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E5%88%B0-github-pages\nhttps://isdaniel.github.io/hexo-blog-theme/\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/start-with-hexo/ Written with Passion and Hope\n","categories":["Web","Frontend","Talk","Blog"],"tags":["Hexo","Github","Github Pages","Web"]},{"title":"Syncthing 文件同步工具部署和 iOS 替代方案","url":"//blog/2020/03/syncthing-all-platform/","content":"Syncthing Official Site: https://syncthing.net/\nSyncthing Github: https://github.com/syncthing/\nKodExplorer Official Site: https://kodcloud.com/\nKodExplorer Github: https://github.com/kalcaddle/KodExplorer\nIntroduction\nSyncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it’s transmitted over the internet.\n\nSyncthing 是一个实时的文件同步程序，可以在两台以上的设备之间进行实时、端对端的文件同步。在不同设备间同步的时候，还可以对每个设备分别设置文件版本控制，保留被删除的文件的副本或者更改前的旧版本，在办公、科研、文档共享和数据共享上都有很大用处。\n我将 Syngthing 安装在了学校电脑、 Surface 和远端服务器上，远端服务器有公网 IP ，方便进行快速同步。这样只要我本地的数据发生更新，远端服务器就会同步数据，并且保存文件的旧版本。\n服务器安装虽然是开源项目，但是我不建议使用源代码安装。源码安装不利于软件的升级和版本控制。\n官网对文件安装的文档说明已经非常详细了，这边简单摘抄一些，并分享在安装过程中发现的问题并提供解决方案。\n全平台（除了 iOS）的安装文件可以在这里找到 https://syncthing.net/downloads/ 。\nDeb 系用户可以在两条 release 轨道中进行选择，分别是”stable” (latest release) 和 “candidate” (earlier release candidate) 。\n同时强烈建议通过 HTTPS 从 apt 源进行下载。\nCandidate Track 的版本升级会比 Stable Track 早大约三个星期，追求刺激的用户可以使用 Candidate Track 。\nStable Track# Add apt https supportsudo apt-get install apt-transport-https# Add the release PGP keys:curl -s https://syncthing.net/release-key.txt | sudo apt-key add -# Add the &quot;stable&quot; channel to your APT sources:echo &quot;deb https://apt.syncthing.net/ syncthing stable&quot; | sudo tee /etc/apt/sources.list.d/syncthing.list# Update and install syncthing:sudo apt-get updatesudo apt-get install syncthing\n\n\n\nCandidate Track# Add apt https supportsudo apt-get install apt-transport-https# Candidate Version# Add the release PGP keys:curl -s https://syncthing.net/release-key.txt | sudo apt-key add -# Add the &quot;candidate&quot; channel to your APT sources:echo &quot;deb https://apt.syncthing.net/ syncthing candidate&quot; | sudo tee /etc/apt/sources.list.d/syncthing.list# Update and install syncthing:sudo apt-get updatesudo apt-get install syncthing\n\nRedHat 系RedHat 系统可以直接在官网下载已经编译好的程序，并直接执行。为了方便执行，可以通过 ln -s 将程序可执行文件软连接到 /usr/bin 下。\nWindows &amp; MacOS可以直接在官网下载到可执行程序，执行后将会自动弹出 Web 页面。\n打通 Syncthing 节点之间的第一条 P2P 连接让我们开始连接第一条端到端同步线路。这条线路将会在本地 PC 和远端服务器之间同步数据。\n我打算将我的服务器作为文件存放的主服务器，一大原因是服务器有公网地址，不需要通过 NAT 穿透来访问，这会大大提高同步速度。我的主服务器是 Debian buster。\n因为 Syncthing 并没有 iOS 客户端。为了在 iOS 上使用 Syncthing ，我采用了一个 Trick，用 KodExplorer 作为网页访问文件的工具，并将同步文件夹放置在 KodExplorer 的目录下。所以，如果想保持 KodExplorer 和 Syncthing 可以正常协同工作，需要单独创建一个用户来运行 Syncthing 和 PHP ，保证他们的 读写权限相同 。同时，官方也建议不要使用 root 账户运行 Syncthing 。\n我使用 www 用户运行 PHP ，所以现在默认我已经切换到 www 账户运行 Syncthing 了。\n我使用 screen 控制程序后台运行。在安装完程序后，执行命令 syncthing 让其在 ~/.config/syncthing 下生成初始配置文件。程序默认监听 127.0.0.1:8384 作为其 Web GUI 页面。如果你有 NginX 或者 Apache ，可以设置一个反向代理到这个端口，就能直接进入网站页面。如果没有，需要 Ctrl + C 终止程序运行，并修改 ~/.config/syncthing/config.xml 中的监听地址为 0.0.0.0 ，然后通过公网 IP 访问该地址进行配置。如果这个页面是暴露在公网上的，请 务必设置管理员密码 。\n远端服务器节点配置完成后，我们开始配置本地 PC 节点。在 PC 中下载程序并运行，在 GUI 页面中点击右下角 添加远程设备 ，输入从远端服务器的 ID（服务器 GUI 界面右上角操作-显示 ID），然后等待一段时间，远端服务器会收到本地 PC 发起的远程设备连接请求，同意即可。\n如果没有收到请求，可以检查一下远端服务器是否打开了端口 22000&#x2F;TCP ，21027&#x2F;UDP 防火墙。\n远端接受请求后，就可以添加本地同步文件夹，并和远端进行共享了。\n理论上说，如果两个设备都在 NAT 设备背后，也可以进行数据同步。但是在测试过程中发现数据同步速度相对缓慢，并且无法用本文的 Trick 让 iOS 设备访问。\nSyncthing 和 KodExplorer 的协同工作为了让这两个程序协作，让 iOS 设备也能访问远端文件，我们需要把同步文件夹放在 KodExplorer 程序的 data/对应用户/home 文件夹下。这样，iOS 设备就可以通过浏览器访问远程文件了。\nKodExplorer 的安装这里不赘述。\n当然，这也会有安全问题。将私人文件放在可以直接被访问到的服务器目录下是一件很危险的行为。这时候需要对 data/ 文件夹进行访问权限控制，在 NginX 配置文件中增加一条即可。\nlocation /data/ &#123;    deny all;&#125;\n\n\n\n这样，在 iOS 设备上，可以直接通过访问 KodExplorer 页面来访问文件，并且能够在多端间进行同步。\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/syncthing-all-platform/ Written with Passion and Hope\n","categories":["Web","Tool"],"tags":["Syncthing","Sync"]},{"title":"Docker 限制磁盘 IO 无效排错及总结","url":"//blog/2020/03/blkio-debug/","content":"\n该文完稿于 2020-03-13 凌晨\n\nBackground最近在做一个与 Docker 相关的实验，其中需要限制 Docker 容器中应用程序的 IO，比如 NginX 的 IO。这听起来很简单，毕竟远在 Feb 4th, 2016 release 的 Docker v1.10 就在其功能中加入了限制容器 IO 的参数\n\nConstraints on disk I&#x2F;O: Various options for setting constraints on disk I&#x2F;O have been added to docker run: --device-read-bps, --device-write-bps, --device-read-iops, --device-write-iops, and --blkio-weight-device.\nhttps://www.docker.com/blog/docker-1-10/\n\n就在一切都顺利进行，我写完包含了 NginX 的 Dockerfile ，准备满心欢喜地开始我的 1MB/s 实验的时候，一道晴天霹雳打在我心上——\nroot@53ace8551c27:/#$ dd if=500M.file bs=1M count=500 of=/dev/null500+0 records in500+0 records out524288000 bytes (524 MB, 500 MiB) copied, 0.132448 s, 4.0 GB/s\n\n当然问题现在已经解决了。为了重现当时的情况，我们从头开始。\nToolbox我们简单地使用 Debian 作为测试的 Docker Image。\n$ docker pull debian\n\n并且使用 dd 命令生成一个 500M 的 000 文件，测试磁盘读写速度。\n$ dd if=/dev/zero of=500M.file bs=1M count=500$ dd if=500M.file bs=1M count=500 of=/dev/null\n\nYesterday Once More *\n* Yesterday Once More – Carpenters\n\n初次碰壁拉镜像\n$ docker pull debianUsing default tag: latestlatest: Pulling from library/debian50e431f79093: Pull completeDigest: sha256:a63d0b2ecbd723da612abf0a8bdb594ee78f18f691d7dc652ac305a490c9b71aStatus: Downloaded newer image for debian:latestdocker.io/library/debian:latest\n\n找到宿主机的设备路径\n$ sudo fdisk -lDisk /dev/vda: 50 GiB, 53687091200 bytes, 104857600 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: -Device      Start       End   Sectors  Size Type/dev/vda1  227328 104857566 104630239 49.9G Linux filesystem/dev/vda14   2048     10239      8192    4M BIOS boot/dev/vda15  10240    227327    217088  106M EFI SystemPartition table entries are not in disk order.\n\n起一个 Docker，限制对应设备的读写速度\n$ docker run -it --rm --device-read-bps /dev/vda:1MB  --device-write-bps /dev/vda:1MB debianroot@9f79e6469b67:/#\n\n$ dd if=/dev/zero of=500M.file bs=1M count=500500+0 records in500+0 records out524288000 bytes (524 MB, 500 MiB) copied, 0.728238 s, 720 MB/s$ dd if=500M.file bs=1M count=500 of=/dev/null500+0 records in500+0 records out524288000 bytes (524 MB, 500 MiB) copied, 0.132448 s, 4.0 GB/s\n\n？\n这是一个非常可怕的事情。我限制的 1MB/s 并不工作。这个实验是基于这个假设进行的，如果没有办法限制设备的 IO，实验也没有办法继续进行了。\n\n\n另辟蹊径为了完成实验，我找了大量的资料。首先我把焦点放在使用 systemd 控制资源限制上。\n根据 systemd 的文档（ https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html#Options ），我们可以在对应服务的 systemd 配置文件中增加一些参数来实现自动化的资源控制，包括 CPU 资源限制，Memory 资源限制，进程数资源限制和 IO 限制。\n\nIOAccounting &#x3D;\n\nTurn on Block I&#x2F;O accounting for this unit, if the unified control group hierarchy is used on the system. Takes a boolean argument. Note that turning on block I&#x2F;O accounting for one unit will also implicitly turn it on for all units contained in the same slice and all for its parent slices and the units contained therein. The system default for this setting may be controlled with DefaultIOAccounting= in systemd-system.conf(5).\nThis setting replaces BlockIOAccounting= and disables settings prefixed with BlockIO or StartupBlockIO.\n\nIOReadBandwidthMax&#x3D;device bytes, IOWriteBandwidthMax&#x3D;device bytes\n\nSet the per-device overall block I&#x2F;O bandwidth maximum limit for the executed processes, if the unified control group hierarchy is used on the system. This limit is not work-conserving and the executed processes are not allowed to use more even if the device has idle capacity. Takes a space-separated pair of a file path and a bandwidth value (in bytes per second) to specify the device specific bandwidth. The file path may be a path to a block device node, or as any other file in which case the backing block device of the file system of the file is used. If the bandwidth is suffixed with K, M, G, or T, the specified bandwidth is parsed as Kilobytes, Megabytes, Gigabytes, or Terabytes, respectively, to the base of 1000. (Example: “&#x2F;dev&#x2F;disk&#x2F;by-path&#x2F;pci-0000:00:1f.2-scsi-0:0:0:0 5M”). This controls the “io.max“ control group attributes. Use this option multiple times to set bandwidth limits for multiple devices. For details about this control group attribute, see IO Interface Files.\nThese settings replace BlockIOReadBandwidth= and BlockIOWriteBandwidth= and disable settings prefixed with BlockIO or StartupBlockIO.\nSimilar restrictions on block device discovery as for IODeviceWeight= apply, see above.\n\n\n这个方法一听就非常靠谱。 systemd 是一个让人又爱又恨的工具，我曾经为了从 service 切换到 systemctl 不知道背了多久这个命令的单词拼写。由于我们需要限制 NginX 的 IO 资源，首先要找到 NginX 的 systemd 配置文件。\n$ sudo systemctl status nginx● nginx.service - A high performance web server and a reverse proxy server   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset:   Active: active (running) since Wed 2020-03-11 14:17:51 UTC; 24h ago     Docs: man:nginx(8)  Process: 3677 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5  Process: 3685 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (co  Process: 3678 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_proces Main PID: 3687 (nginx)    Tasks: 17 (limit: 4915)   CGroup: /system.slice/nginx.service           ├─3687 nginx: master process /usr/sbin/nginx -g daemon on; master_p           ├─3688 nginx: worker process           ...\n\n然后进入 /lib/systemd/system/nginx.service ，修改 [Service] 块，增加三行。\nIOAccounting=trueIOReadBandwidthMax=/dev/vda 1MIOWriteBandwidthMax=/dev/vda 1M\n\nReload daemon and nginx。\n$ sudo systemctl daemon-reload$ sudo systemctl restart nginx\n\n由于我在 NginX 的网页目录下放了一个测试下载速度的文件，我换了内网其他机器去拉。\n$ wget http://10.10.194.18/dash/test.file--2020-03-12 15:07:57--  http://10.10.194.18/dash/test.fileConnecting to 10.10.194.18:80... connected.HTTP request sent, awaiting response... 200 OKLength: 104857600 (100M) [application/octet-stream]Saving to: ‘test.file’test.file           100%[=================&gt;] 100.00M   339MB/s    in 0.3s2020-03-12 15:07:57 (339 MB/s) - ‘test.file’ saved [104857600/104857600]\n\n这个结果无疑告诉我这次尝试又失败了。\n曙光初现这个问题真的很奇怪，为什么我对资源的限制会不起作用。我后来在 NginX 的 systemd 配置文件中增加了 Memory 的 limit 是 work 的，但是 IO 相关的就不行。在本地虚拟机测试中，不管是 Docker 的 IO 限制还是 NginX 的 systemd 资源控制都是生效的，甚至一度让我怀疑是远程服务器的镜像问题。因为根据我咨询运维人员的情况来看，远程机器的镜像都经过特殊定制，有可能是这个原因。\n不过，后来我的 Teammate 给了我一个 很重要的提示 。通过给 dd 命令增加参数 oflag=direct ，在 Docker 中可以得到限速后的效果。\nroot@9f79e6469b67:/# dd if=500M.file bs=1M count=500 of=500.out oflag=direct^C18+0 records in18+0 records out18874368 bytes (19 MB, 18 MiB) copied, 18.0052 s, 1.0 MB/s\n\n这个参数的作用是什么呢？GNU https://www.gnu.org/software/coreutils/manual/html_node/dd-invocation.html#dd-invocation 介绍如下\n\n‘oflag&#x3D;flag[,flag]…’\nAccess the output file using the flags specified by the flag argument(s). (No spaces around any comma(s).)\nHere are the flags. Not every flag is supported on every operating system.\n\n‘direct’\nUse direct I&#x2F;O for data, avoiding the buffer cache. Note that the kernel may impose restrictions on read or write buffer sizes. For example, with an ext4 destination file system and a Linux-based kernel, using ‘oflag&#x3D;direct’ will cause writes to fail with EINVAL if the output buffer size is not a multiple of 512.\n\n\n这说明远程服务器的读写缓存对硬盘 IO 产生了巨大的影响。虽然我记得之前不知道在哪里看到过说，Cgroups 的磁盘 IO 限制模块 blkio 会对读写 buffer 进行限制，但是由于这是远程服务器，并且镜像经过定制，可能在系统底层绕开了这一限制，用于提升服务器 IO。\n我也曾经考虑过读写 buffer 的问题，但是当时执行命令清空读写缓存时，遇上了这样的问题\n$ sudo echo 1 &gt; /proc/sys/vm/drop_caches-bash: /proc/sys/vm/drop_caches: Permission denied\n\n我便没有继续。\n问题解决最终，我们把问题锁定在服务器的读写缓存上。既然已经知道了问题所在，解决起来也就相对容易。虽然没有办法直接将清除缓存命令写进特定位置，但是可以用这条命令解决。\n$ sudo sh -c &quot;/bin/echo 1 &gt; /proc/sys/vm/drop_caches&quot;\n\n这是工作的。至于为啥，我没研究。\n还有另一套方案，将磁盘缓存的超时时间设置极低，也可以解决。\n$ sudo echo 100 &gt; /proc/sys/vm/dirty_expire_centisecs$ sudo echo 100 &gt; /proc/sys/vm/dirty_writeback_centisecs\n\n当然，这些命令都要在宿主机进行操作，因为 Docker 本质只是一个在宿主机上虚拟化的线程。/proc/sys/vm/ 文件夹对 Docker 容器来说，只是一个 Read-Only 的文件系统。\nroot@9f79e6469b67:/# echo 100 &gt; /proc/sys/vm/dirty_writeback_centisecsbash: /proc/sys/vm/dirty_writeback_centisecs: Read-only file system\n\n最终，在清除缓存后，一切都变得正常起来。\nroot@9f79e6469b67:/# dd if=500M.file bs=1M count=500 of=/dev/null^C44+0 records in43+0 records out45088768 bytes (45 MB, 43 MiB) copied, 44.4906 s, 1.0 MB/s\n\n$ wget 10.10.194.18/dash/test.file--2020-03-12 15:42:29--  http://10.10.194.18/dash/test.fileConnecting to 10.10.194.18:80... connected.HTTP request sent, awaiting response... 200 OKLength: 104857600 (100M) [application/octet-stream]Saving to: ‘test.file.1’test.file.1         100%[=================&gt;] 100.00M  1005KB/s    in 1m 45s2020-03-12 15:44:14 (977 KB/s) - ‘test.file.1’ saved [104857600/104857600]\n\nAppendix我觉得这篇文章要是就这么结束，内容应该有点太少了。在填坑过程中，我还研究了不少和系统资源控制相关的内容。\nCgroupCgroup 是 Linux 用于控制进程资源的一种方式，从 2.6.24 内核中开始搭载，v2 版本于 4.5 内核开始搭载。它的配置文件在文件系统中的组织方式是 /sys/fs/cgroup/&#123;Resource&#125;/&#123;defaultConfigs&#125; 和 /sys/fs/cgroup/&#123;Resource&#125;/&#123;Groups&#125;/.../&#123;configs&#125; 。对应的限制内容会被写在目录的文件下，限制进程的 pid 会被写在目录的 tasks 文件夹下。简单看看本文主角 blkio 文件夹下的结构。\n/sys/fs/cgroup/blkio$ lsblkio.io_merged                   blkio.throttle.io_servicedblkio.io_merged_recursive         blkio.throttle.read_bps_deviceblkio.io_queued                   blkio.throttle.read_iops_deviceblkio.io_queued_recursive         blkio.throttle.write_bps_deviceblkio.io_service_bytes            blkio.throttle.write_iops_deviceblkio.io_service_bytes_recursive  blkio.timeblkio.io_service_time             blkio.time_recursiveblkio.io_service_time_recursive   blkio.weightblkio.io_serviced                 blkio.weight_deviceblkio.io_serviced_recursive       cgroup.clone_childrenblkio.io_wait_time                cgroup.procsblkio.io_wait_time_recursive      cgroup.sane_behaviorblkio.leaf_weight                 dockerblkio.leaf_weight_device          notify_on_releaseblkio.reset_stats                 release_agentblkio.sectors                     system.sliceblkio.sectors_recursive           tasksblkio.throttle.io_service_bytes   user.slice\n\n之前我们修改的 systemd/nginx.service 的内容被放在 system.slice 下，docker 的资源限制被放在 docker/ 和 docker/container_id 下。\n看看 cgroup 支持哪些资源\n$ lssubsys  -mcpuset /sys/fs/cgroup/cpusetcpu,cpuacct /sys/fs/cgroup/cpu,cpuacctblkio /sys/fs/cgroup/blkiomemory /sys/fs/cgroup/memorydevices /sys/fs/cgroup/devicesfreezer /sys/fs/cgroup/freezernet_cls,net_prio /sys/fs/cgroup/net_cls,net_prioperf_event /sys/fs/cgroup/perf_eventhugetlb /sys/fs/cgroup/hugetlbpids /sys/fs/cgroup/pidsrdma /sys/fs/cgroup/rdma\n\n简单验证一下生效的几个配置。\nDocker 资源限制Docker ID: 9f79e6469b67\n$ docker inspect 9f79e6469b67 | grep Pid            &quot;Pid&quot;: 6510,            &quot;PidMode&quot;: &quot;&quot;,            &quot;PidsLimit&quot;: null,\n\n$ cd /sys/fs/cgroup/blkio/docker/9f79e6469b67... $ lsblkio.io_merged                   blkio.sectors_recursiveblkio.io_merged_recursive         blkio.throttle.io_service_bytesblkio.io_queued                   blkio.throttle.io_servicedblkio.io_queued_recursive         blkio.throttle.read_bps_deviceblkio.io_service_bytes            blkio.throttle.read_iops_deviceblkio.io_service_bytes_recursive  blkio.throttle.write_bps_deviceblkio.io_service_time             blkio.throttle.write_iops_deviceblkio.io_service_time_recursive   blkio.timeblkio.io_serviced                 blkio.time_recursiveblkio.io_serviced_recursive       blkio.weightblkio.io_wait_time                blkio.weight_deviceblkio.io_wait_time_recursive      cgroup.clone_childrenblkio.leaf_weight                 cgroup.procsblkio.leaf_weight_device          notify_on_releaseblkio.reset_stats                 tasksblkio.sectors$ cat tasks6510$ cat blkio.throttle.read_bps_device252:0 1048576\n\n其中，252 : 0 是磁盘设备号 &lt;major&gt;:&lt;minor&gt; ，1048576 &#x3D; 1024 * 1024\n$ ls -l /dev/vdabrw-rw---- 1 root disk 252, 0 Mar 11 13:32 /dev/vda\n\n这说明在我们启动一个资源受限的 Docker 时，Docker 会自动在自身 cgroup 资源限制组下生成名为 container_id 的文件夹，然后将对应容器的 Pid 和限制资源规则写入。\nSystemd 资源限制$ cd /sys/fs/cgroup/blkio/system.slice/nginx.service$ cat tasks66486651...\n\n$ cat blkio.throttle.read_bps_device252:0 1000000$ sudo systemctl status nginx● nginx.service - A high performance web server and a reverse proxy server   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset:   Active: active (running) since Thu 2020-03-12 15:06:14 UTC; 1h 5min ago     Docs: man:nginx(8)  Process: 6632 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5  Process: 6646 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (co  Process: 6633 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_proces Main PID: 6648 (nginx) ...\n\nNginX 的所有进程 Pid 和 .../system.slice/nginx.service/tasks 中的 Pid 一一对应。\nsystemd 的资源限制工作方式和 Docker 类似。\n有趣的是，Docker 采用 2 ^ 10  作为单位，而 systemd 采用 1000 作为单位。\nReferences\n[Systemd] https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html#Options\n[dd] https://www.gnu.org/software/coreutils/manual/html_node/dd-invocation.html#dd-invocation\n[Disk Cache] https://stackoverflow.com/questions/20215516/disabling-disk-cache-in-linux\n[Disk Cache] https://unix.stackexchange.com/questions/48138/how-to-throttle-per-process-i-o-to-a-max-limit\n[Disk Cache] https://unix.stackexchange.com/questions/109496/echo-3-proc-sys-vm-drop-caches-permission-denied-as-root\n[Cgroup] https://coolshell.cn/articles/17049.html\n[Cgroup] https://tech.meituan.com/2015/03/31/cgroups.html\n[Cgroup] https://cizixs.com/2017/08/25/linux-cgroup/\n[Cgroup blkio] https://andrestc.com/post/cgroups-io/\n[Cgroup blkio] https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt\n[Docker IO] https://stackoverflow.com/questions/36145817/how-to-limit-io-speed-in-docker-and-share-file-with-system-in-the-same-time\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/blkio-debug/ Written with Passion and Hope\n","categories":["Linux","Docker"],"tags":["Docker","blkio","Cgroups"]},{"title":"Customize Hexo and Theme NexT","url":"//blog/2020/03/customize-hexo/","content":"Introduction我大概是从两天前开始决定使用 Hexo 的。\n橙橙橙 2020-03-10 15:35:42    你现在在用hexo吗锅 2020-03-10 16:15:22    对\n\n橙橙橙 2020-03-12 18:05:24    你觉得hexo好还是jekyll好mxd 2020-03-12 18:12:17    我喜欢hexo\n\n昨天我装上了 Hexo ，配置了 gh-pages ，今天我装了一堆插件用于优化浏览体验，并把主题改成了自己喜欢的样子。我还增加了 gitalk 评论系统，并接入了 Google Analytics 服务等传统艺能。\n我会在这个帖子里做一些记录，关于我修改的比较重要的部分。\n修改 Hexo 字体我修改了 Hexo 全局的字体，并根据自己的审美修改了字体大小和排版。修改字体需要重写 CSS ，覆盖 NexT 原有的样式表。\n在 NexT 的配置文件中找到如下片段\n# Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path:  #head: source/_data/head.swig  #header: source/_data/header.swig  #sidebar: source/_data/sidebar.swig  #postMeta: source/_data/post-meta.swig  #postBodyEnd: source/_data/post-body-end.swig  #footer: source/_data/footer.swig  #bodyEnd: source/_data/body-end.swig  #variable: source/_data/variables.styl  #mixin: source/_data/mixins.styl  style: source/_data/styles.styl\n\n由于我只自定义了样式表，所以我只是把样式表的部分取消注释。我希望修改我的网站全局的字体，所以我先通过 https://www.font-converter.net/ 将 ttf 字体转换成了多种格式，将其放在 source/fonts 文件夹下，并增加 CSS 配置。\n@font-face &#123;  font-family: &quot;FZS3JW&quot;;  src: url(&quot;/fonts/FZS3JW/FZS3JW.eot&quot;); /* IE9 Compat Modes */  src: url(&quot;/fonts/FZS3JW/FZS3JW.eot?#iefix&quot;) format(&quot;embedded-opentype&quot;), /* IE6-IE8 */    url(&quot;/fonts/FZS3JW/FZS3JW.otf&quot;) format(&quot;opentype&quot;), /* Open Type Font */    url(&quot;/fonts/FZS3JW/FZS3JW.svg&quot;) format(&quot;svg&quot;), /* Legacy iOS */    url(&quot;/fonts/FZS3JW/FZS3JW.ttf&quot;) format(&quot;truetype&quot;), /* Safari, Android, iOS */    url(&quot;/fonts/FZS3JW/FZS3JW.woff&quot;) format(&quot;woff&quot;), /* Modern Browsers */    url(&quot;/fonts/FZS3JW/FZS3JW.woff2&quot;) format(&quot;woff2&quot;); /* Modern Browsers */  font-weight: normal;  font-style: normal;  font-display: swap;&#125;body &#123;    font-family: &quot;Times New Roman&quot;, &quot;FZS3JW&quot;, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;;&#125;\n\n这样，全局的字体就被修改为上述优先级了。\n评论NexT 提供了非常方便的评论接口，只需要申请对应的评论服务并把他们的 API Key 或者别的相关的东西复制过来就可以了。我这里主要记录一下 gitalk 的安装过程。\nGitalkgitalk 是一个使用 GitHub issue 作为对话记录工具的评论，我觉得想到这个点子的人一定是天才。\n首先我们需要创建一个 repo ，用于放所有的 issue （评论）。我创建了一个名为 gitalk-wasteland 的 repo 。然后我们什么都不用操作，进行下一步。\n进入 GitHub OAuth App 申请页面 https://github.com/settings/applications/new ，创建一个新的 OAuth App 并设置 Homepage URL 和 Authorization callback URL 为 网站首页 URL 。然后根据配置文件注释，将 Client ID ， Client Secret ， gitalk-wasteland(repo name) 等信息填入\n\n\ngitalk:  enable: true  ...\n\n即可生效。这个评论模块无法在本地测试，需要部署到服务器后在之前填入的 URL 下进行测试。\n不要忘记打开\n# Multiple Comment System Supportcomments:  # Available values: tabs | buttons  style: tabs  # Choose a comment system to be displayed by default.  # Available values: changyan | disqus | disqusjs | gitalk | livere | valine  active: gitalk  ...\n\n插件搜索按钮 hexo-generator-searchdbGitHub: https://github.com/theme-next/hexo-generator-searchdb  \nInstall$ yarn add hexo-generator-searchdb\n\nConfiguresearch:  path: search.xml  field: post  content: true  format: html\n\nUpdate$ yarn upgrade hexo-generator-searchdb\n\nSPA 插件 PJAX for NexTGitHub: https://github.com/theme-next/theme-next-pjax  \n在用户点击网页内链接时，通过 AJAX 的方式拉取新页面并渲染，达到 SPA (Single Page Application) 的效果。\nInstall$ cd themes/next$ git clone https://github.com/theme-next/theme-next-pjax source/lib/pjax\n\nBut I recommend this\n$ git submodule add https://github.com/theme-next/theme-next-pjax source/lib/pjax\n\nConfigurepjax: true\n\nUpdate$ cd themes/next/source/lib/pjax$ git pull\n\n网页加载进度条 Progress bar for NexTGitHub: https://github.com/theme-next/theme-next-pace  \n这个插件可以在页面元素加载的时候给页面增加一个进度条提示。\nInstall$ cd themes/next$ git clone https://github.com/theme-next/theme-next-pace source/lib/pace\n\nI recommend this as well\n$ git submodule add https://github.com/theme-next/theme-next-pace source/lib/pace\n\nConfigure在 NexT 配置文件中启用该模块\npace:  enable: true  # Themes list:  # big-counter | bounce | barber-shop | center-atom | center-circle | center-radar | center-simple  # corner-indicator | fill-left | flat-top | flash | loading-bar | mac-osx | material | minimal  theme: minimal\n\nUpdate$ cd themes/next/source/lib/pace$ git pull\n\nSitemap hexo-generator-sitemapGitHub: https://github.com/hexojs/hexo-generator-sitemap  \nInstall$ yarn add hexo-generator-sitemap\n\nConfigure添加设置\nsitemap:  path: sitemap.xml  template: ./path/to/sitemap_template.xml  rel: false\n\n添加 Sitemap 模板文件于你喜欢的位置\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;urlset xmlns=&quot;http://www.sitemaps.org/schemas/sitemap/0.9&quot;&gt;  &#123;% for post in posts %&#125;  &lt;url&gt;    &lt;loc&gt;&#123;&#123; post.permalink | uriencode &#125;&#125;&lt;/loc&gt;    &#123;% if post.updated %&#125;    &lt;lastmod&gt;&#123;&#123; post.updated.toISOString() &#125;&#125;&lt;/lastmod&gt;    &#123;% elif post.date %&#125;    &lt;lastmod&gt;&#123;&#123; post.date.toISOString() &#125;&#125;&lt;/lastmod&gt;    &#123;% endif %&#125;  &lt;/url&gt;  &#123;% endfor %&#125;&lt;/urlset&gt;\n\n对于不想被 Sitemap 包含的文件，可以在头部写入 sitemap: false 。\nUpdate$ yarn upgrade hexo-generator-sitemap\n\nRSS hexo-generator-feedGitHub: https://github.com/hexojs/hexo-generator-feed\nInstall$ yarn add hexo-generator-feed\n\nConfigfeed:  type: atom  path: atom.xml  limit: 0  hub:  content:  content_limit: 500  content_limit_delim: &#x27; &#x27;  order_by: -date  icon: icon.png  autodiscovery: true  template:\n\nUpdate$ yarn upgrade hexo-generator-feed\n\n最后废话两句网上很多教程会让你直接修改主题内的文件。这不是不行，但是主题作者已经尽量将配置文件暴露，便于主题升级。\n这其中也包括对网站增加插件。举个例子，对于插件 theme-next-pjax ， README.md 中写的是将插件下载到 next/source/lib 下，但我的建议是用 git submodule add ，将其作为一整个包插入，也方便以后升级维护。\nNexT 主题解耦合可以把 NexT 的配置文件放置在 source/_data/next.yml 下，然后修改该配置文件 override: true 即可将设置完全覆盖。对于自己增加的 CSS 、 JS 等文件，也可以放在 NexT 配置文件中提到的对应位置，这对今后的升级和维护有很大帮助。\n如果已经在使用 NexT 主题，确保主题文件夹的修改都备份好的情况下，执行 git rm themes/next ，从 git 中删除并保留主题原文件。然后执行\n$ git submodule add https://github.com/theme-next/hexo-theme-next themes/next\n\n通过增加 submodule 的方式增加主题，并且今后不要再对 themes/next 文件夹下的文件做任何修改，否则可能产生一个 dirty commit 。\n我在通过 submodule add 增加 PJAX, pace 的时候遇上了下面这个问题\nYAMLException: end of the stream or a document separator is expected at line 9, column 102:     ... languages` and other directories:                                         ^    at generateError (C:\\Projects\\Website\\wasteland\\node_modules\\js-yaml\\lib\\js-yaml\\loader.js:167:10)    ...\n\n遇上这种问题，我推测是执行相关命令时， Hexo 会对 source/ 下的所有特定后缀文件进行语法检查。我通过全局搜索含有 languages and other directories: 的文件定位到之前添加的 submodule 中 README.md 存在非法（不符合 YAML 语法检查器）的内容。\n添加配置跳过这些文件就可以解决这个问题了。\nexclude:     - lib/**/*.md\n\nReferences\n关于@font-face加载前空白(FOIT)的解决方案 - https://juejin.im/post/5a7587d4f265da4e8a31c213  \nHexo中Gitalk配置使用教程-可能是目前最详细的教程 - https://iochen.com/2018/01/06/use-gitalk-in-hexo/\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/customize-hexo/ Written with Passion and Hope\n","categories":["Web","Frontend","Blog"],"tags":["Hexo","Hexo Theme"]},{"title":"使用 LaTeX 完成你的毕业论文","url":"//blog/2020/03/write-thesis-with-latex/","content":"本文使用 武汉大学 毕业论文模板。\n准备写论文可喜可贺，我终于要开始动笔我的毕业论文了。虽然论文相关的实验和理论准备已经就绪了很长一段时间，但是就是提不起精神写文章。这周其他突发堆积的事情逐渐清空，我准备忙里偷闲，开始写毕业论文。\n某天，一同学在群里发了一份 武汉大学毕业论文 LaTeX 模板，我一看，巧了，老朋友写的 repository ，于是正好近期开工，把它下载下来用。\n环境配置\n本文基础环境是 Windows 10，使用的 IDE 为 VSCode，Linux、Mac 配置方式类似\n\nPerl首先我们需要安装 Windows 下的 Perl 依赖。前往官网 https://www.perl.org/get.html 我发现，有 ActiveState Perl 和 Strawberry Perl 两种版本的 Perl ，具体使用起来差不多，所以我选择了下载起来相对方便的 草莓 Perl http://strawberryperl.com/ 。\nLaTeXWindows 下有两种广泛使用的 LaTeX 版本，分别是 TeX Live 和 MiKTeX 。 TeX Live 安装体积高达 6GB ，我立刻放弃，选择了安装体积较小，后续使用中按需下载的 MiKTeX https://miktex.org/download 。建议在安装时安装给当前用户，可以避免很多麻烦。\n\n\n\n\n同时，我们需要手工指定镜像。根据我的使用情况， HIT 的源中包含了很多不可用的文件，会拖慢下载速度。打开 MiKTeX 客户端，进入 Settings - General - Package Installation 更换源。推荐更换为 清华源 。\n\n\nVSCode首先需要在 VSCode 中安装依赖 LaTeX Workshop 。\nVSCode 的配置文件在上述模板中包含。但是我建议对配置文件进行一定的修改，使用更方便的安装命令。并将工程和 pdf 文件构建于 build/ 文件夹下，保持根目录干净。\n可以在 https://gist.github.com/HoshinoTouko/a2332b1756996d5e9c71605d0ff7591a 获取 .vscode/settings.json 和 .latexmkrc ，下载后放置于对应目录即可工作。\n手工执行也可以通过手工执行的方式对工程进行编译。下载上文的 .latexmkrc 放置于根目录，如果无法访问 gist ，也可以复制下方文件。然后确认自己的 PowerShell 工作于 UTF-8 编码下。确认 PowerShell 编码可以执行 chcp 命令。\n.latexmkrc$pdflatex=q/xelatex -synctex=1 %O %S/\n\n确认 PowerShell 编码PS C:\\Research\\undergraduate-thesis&gt; chcpActive code page: 65001\n\n如果 Active code page 是 65001 ，则该 PowerShell 运行于 UTF-8 编码下。如果不是，执行 chcp 65001 即可。如需更改默认 PowerShell 编码，可以通过以下方式。\n\n\n执行 \n使用 latexmk 调用 xelatex 编译 LaTeX 文档latexmk -xelatex -pdf -synctex=1 -interaction=nonstopmode -file-line-error -shell-escape -outdir=build ./main\n\n即可在 build/ 文件夹下完成工程构建。\n写在最后其实我本来是想用 overleaf 的，但是觉得学位论文这种东西，在线保存有一定风险，还是握在自己手里比较好。所以我就捣鼓了几个小时，把环境都部署好跑起来了。本文提供的样例是武汉大学的，在 GitHub 或者 Google 根据关键词搜索，即可找到别的学校的同学制作的相关模板。例如\n\n浙江大学毕业设计&#x2F;论文 LaTeX 模板 - https://github.com/TheNetAdmin/zjuthesis包含本科生、硕士生与博士生模板，以及英文硕博士模板\nTsinghua University Thesis LaTeX Template - https://github.com/xueruini/thuthesis\nfduthesis - https://github.com/stone-zeng/fduthesis\n上海交通大学 XeLaTeX 学位论文及课程论文模板 - https://github.com/sjtug/SJTUThesis\n…\n\n希望大家度过一个特殊又充实的毕业论文假期\nReferences\n武汉大学毕业论文 LaTeX 模版 2019 - https://github.com/mtobeiyf/whu-thesis\nHow to make LaTeXmk work with XeLaTeX and biber - https://tex.stackexchange.com/questions/27450/how-to-make-latexmk-work-with-xelatex-and-biber\nUsing Latexmk - https://mg.readthedocs.io/latexmk.html\nHow do I run bibtex after using the -output-directory flag with pdflatex, when files are included from subdirectories? - https://tex.stackexchange.com/questions/12686/how-do-i-run-bibtex-after-using-the-output-directory-flag-with-pdflatex-when-f\nCTAN 镜像使用帮助 - https://mirror.tuna.tsinghua.edu.cn/help/CTAN/\nLaTeX-Workshop GitHub - https://github.com/James-Yu/LaTeX-Workshop\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/03/write-thesis-with-latex/ Written with Passion and Hope\n","categories":["Talk"],"tags":["LaTeX","Paper","Thesis"]},{"title":"家庭网络管理(1): 组装一台千元 NAS","url":"//blog/2020/06/home-network-1-DIY-NAS/","content":"心血来潮我的 surface pro 4 实在是太老了，发热也越发严重，最近甚至经常过热关机。于是前段时间花了 2k 装了台能跑现有不少大型网游的主机，装的我非常愉悦。这台主机用的 AMD 平台，鲁大师能跑24w+，目前正在逐渐成为我的主力用机。装了一台就想装第二台。我做了些简单的调研，决定用 1k 左右的预算装一台功耗较低，性能足够的 NAS。\n目前我已经用 NAS 实现了单臂软路由、BT 下载、PLEX、DNS 等一系列功能。今后我会慢慢更新这块内容。\n配置选择由于给自己开了一个不算太高的预算，所以一切资源都要精打细算。首先列出我的需求\n\n占地空间较小\n千兆板载网卡\nSATA 至少 4 个\n功耗较低\nPCIE 能插万兆\n核显\n有升级空间\n\nCPU首先选择 CPU。有朋友在微博推荐 200GE 板 U 套装，这里也推荐，挺不错，只要 500，性能和价格相符，TDP 也只有 35W。但是因为前段时间已经装了一台 AMD 的机子了，CPU 就不选择 AMD 的了。经过各种对比，选择了低功耗六代 i3-6100T。原本打算考虑奔腾 G5600T，但是掐指一算发现超预算了。我是盯着天梯图和 TDP 找的。后来发现在实际使用中，不一定要买低功耗版本，因为很多情况下，Intel 只是对其做了功耗上限的限制，其他和标压版本相差无几。高性能的 U 在运行相同的计算任务的时候，理论上功耗也差不多，所以 TDP 并不是一个需要非常关注的点。\n主板和机箱主板和机箱需要一起考虑。由于我并不打算上赛扬 J 平台，ITX 的主板价格也很贵，所以就无缘 ITX 了。于是我考虑 m-ATX 的主板和机箱。机箱我看中了金河田的 预见 N1 。这个机箱非常小巧，整体只有 202 * 328 * 260mm ，支持 ATX 电源，2 个 3.5 盘位 + 2 个 2.5 or 1 个 3.5 盘位。如果电源用 SFX 或者直流，还能再装 2-4 个 3.5 硬盘。支持一个机箱散热风扇。综合考虑，我选择了这款机箱。\n选择机箱后，我开始考虑主板。为了给以后升级 8-9 代 U 留出空间，根据机箱要求的主板大小（最大 230*185mm），我选择了支持 6、7 代，能刷 BIOS 上 8、9 代 CPU 的 技嘉 B250M-D2VX-SI （226 * 185 mm）。\n内存和其他内存没什么好说的，枭鲸前段时间搞活动，89 RMB 的 DDR4 2666 8G 条。希望能够长期稳定运行。\n其他配件都是零零散散买起来的，有刚好或者喜欢的就买了。\n最终配置单\n\n\n\n\n部件\n型号\n入手渠道\n价格\n备注\n\n\n\n主板\n技嘉 B250M-D2VX-SI\ntb\n165\n目测网吧拆机\n\n\nCPU\nIntel i3-6100T\ntb\n315\n\n\n\n内存\n枭鲸 8G DDR4 2666 普条\npdd\n89\n打折抢的，平时好像 119\n\n\n电源\n先马 破坏神 300W\njd\n85.9\n\n\n\n机箱\n金河田 预见 N1\njd\n129\n\n\n\nSSD\n影驰 铁甲战将 120G\njd\n109\n\n\n\nHDD\n1T * 2\n奶鱼送的\n0\n按 1T 100 算，200\n\n\n散热\n酷冷至尊 夜鹰\njd\n16.9\n\n\n\n风扇\n先马 游戏风暴\njd\n9.4\n\n\n\n转接头\n山泽 DVI to HDMI\njd\n6.9\n\n\n\n合计\n\n\n926.1\n\n\n\n装机配件一览\n\n金河田这款机箱采用了电源前置设计，用内置的电源线连接到机箱后部。这种设计降低了机箱的整体高度，充分利用空间。\n\n\n这个风扇是之前装台式机剩下的。红 + 黑的配色很漂亮（虽然装在机箱里就看不到了），运行时的噪音也很小。反正功耗不大，我就装了一个图一乐。\n\n\n便宜的散热器，没什么特殊的地方。\n\n\n奶鱼送了我俩 1T 的机械硬盘，加上我自己配的固态做缓存，运行 UNRAID 系统就已经很舒服了。机箱顶部有一个硬盘架，用减震螺丝固定，倒挂在机箱中。图中的机械硬盘已经装在硬盘架上了。\n\n\n内存是 pdd 打折抢的枭鲸普条，反正终身保固，也不怕翻车。我在台式机上用的也是枭鲸的条子，目前用起来一切正常，就是不太敢超频。\n组装\n\n先把 CPU 怼上主板。\n\n\n安装散热器底座，涂硅脂，上散热。不得不说这种下压式的散热器安装十分困难，需要大力向下压。向下压散热器的时候我特别害怕大力出奇迹，把主板压断了。\n\n\n装完散热器之后，上内存，接线，不合盖点亮测试良好，最后上硬盘。整机安装大概花了一个多小时。我对这套平台的性能信心很足，估计跑个黑群晖+软路由，再加一系列的 Docker 应用不是问题。\n功耗情况实际测试中，所有硬盘都启动的功耗大约在 35W 左右，CPU 满负载功耗大约 55W。这是一个我可以接受的范围。\n之后我将安装（破解的） UNRAID 系统，会在下一篇文章继续记录。\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/06/home-network-1-DIY-NAS/ Written with Passion and Hope\n","categories":["Network","NAS"],"tags":["NAS"]},{"title":"家庭网络管理(3): UNRAID 安装黑群晖和其他常用软件","url":"//blog/2020/06/home-network-3-hack-synology-and-more/","content":"NAS 该用来做什么我打算将 NAS 定位为一个家庭用的小型多媒体平台，并作为软路由处理流量。因此， NAS 应该要能满足我挂 BT 下载的需求，并且还要能够直接读取 BT 下载的内容，以家庭影院的形式展现。经过筛选，我选择安装以下 Docker 软件\n\nTransmission （BT 下载，Web UI 控制）\nPlex Media Server （家庭影院）\nPi-Hole （DNS 广告过滤和自定义 Host）\nV2Ray （公网访问内网，等于 VPN）\nNginX （统一反代 Web UI，管理端口暴露规则）\n\n以及，感谢 UNRAID 对 kvm 的良好支持，我还安装了\n\n黑群晖\n\n黑群晖群晖是一个卖软件的 NAS 硬件商，其软件十分好用，新手友好。自然，我先选择安装破解的群晖系统。实际上，装完群晖后，我几乎没有使用里面的功能，因为我还是比较倾向于用 UNRAID 自带的 Docker 完成我需要的服务。不过我在研究黑群晖安装的过程中，学习了一些经验，这里做一下记录。\n安装准备\n\n安装黑群晖的原理是安装一个第三方的、破解后的引导，来欺骗群晖系统镜像正常工作。因此我们需要下载破解的引导作为启动盘，然后去官方网站上下载安装镜像就可以了。破解的引导我推荐在 https://xpenology.club/downloads/ 下载。网站中 DSM 6.2指的是支持 DSM 6.2 的系统，引导的文件名最后一串为群晖的机型。在安装时注意下载对应系统和机型的镜像。可以在 https://archive.synology.com/download/DSM/ 下载。\n由于我的虚拟机无法正常启动 Jun’s Loader v1.04b DS918+ ，这里以 Jun’s Loaders DSM 6.2 - Jun’s Loader v1.03b DS3617xs 为例。下载对应引导并解压，将引导拷贝至 NAS。我创建了一个共享文件夹 HackSynology 用于放置群晖相关文件。\n\n\n\n\n\n\n然后到群晖官网，下载对应的系统镜像。在此处样例中，我下载了 DSM_DS3617xs_23739.pat 文件。将这两个文件准备好，就可以开始安装黑群晖了。\n\n\n创建虚拟机\n\n创建一个 Linux 虚拟机，命名，选择直通的 CPU 核心。UNRAID 的这点我很喜欢，它能够手动指定虚拟机使用其中的某几个核心，避免虚拟机大量占用资源。这里我选择了 CPU 1 作为黑群晖的核心。RAM 1-2GB 就够了。由于兼容性问题，此处需要选择 i440fx-4.2 和 Sea-BIOS 。\n\n\n启动引导和挂载硬盘如图所示设置。其中挂载硬盘的大小可以按照个人喜好修改，总线填写 SATA，类型为 qcow2 ，这样动态分配空间，不需要创建硬盘时就占用对应容量。主硬盘指向下载的破解启动引导，总线类型为 USB。\n\n\n网卡随意，手动关闭最下方的 Start VM atfer creation 并保存。\n\n\n\n\n重新进入编辑页面，点击右上方的 FORM VIEW ，转为 XML VIEW ，搜索 &lt;model type= ，将虚拟网卡型号修改为 e1000 或 vmxnet3 （理论性能更佳，未验证）。\n\n\n此时可以保存，启动机器。\n安装\n\n如果一切正常，VNC 中可以看到上方的情况。此时可以打开 http://find.synology.com/ ，搜索内网的群晖新机器地址。如果找不到，可能需要从 https://www.synology.com/en-global/support/download 下载 Synology Assistant 进一步寻找。\n\n\n\n\n正常进行安装流程。当需要安装系统镜像时，选择手动安装，上传从官网下载的系统镜像。一定要保证版本和机型对应。如果无法安装系统镜像，首先检查版本是否正确，如果都没问题，可以考虑更换破解引导的版本。我一开始测试的是 DS918+ 机型的破解引导，后来发现不能工作，于是更换了现在的 DS3617xs 。\n\n\n安装完成后，如果 Web 界面显示重启，黑群晖就成功以虚拟机的形式被安装了。之后进入群晖系统，进行初期配置。群晖的交互足够用户友好，不存在什么困难，这里就不再赘述。\n\n\n其他 Docker 应用Transmission (with web UI)\n\nTransmission 的安装很容易。进入 Apps 搜索 Transmission，找到作者是 linuxserver 的，下载即可。我简单提供一份配置样本。\n\n\n9091 端口是 Transmission 的 Web UI，&#x2F;downloads 是默认下载文件夹，其他按照默认配置就好。值得一提的是，我额外增加了一个参数 TRANSMISSION_WEB_HOME ，设置为 /transmission-web-control/ ，这是一个自带的 Web 界面美化插件，类似一个主题，添加 Variable 变量即可。\n\n\n安装完成后，可以从 Docker - Transmission 进入 Web UI，添加下载或是管理做种了。\nPLEX主程序安装\n\nPLEX 在 Apps 中自带官方打包好的版本，直接安装即可。\n\n\n设置可以参考上图。其中 Key 1: PLEX_CLAIM 需要在官网 https://www.plex.tv/ 注册后，进入 https://plex.tv/claim 申请。PLEX Claim ID 有效期很短，申请后建议立刻启动服务器，进入 Web UI 界面完成绑定流程。失效了也没有关系，重新进入 claim 页面申请、重新填写 Docker 设置中的变量就可以了。\n值得一提的是，为了将 Transmission 下载的内容直接导入 PLEX ，我将 Transmission 下载文件夹对应的宿主机文件夹同时挂载在 PLEX 对应的数据文件夹下。可以参考设置中最后一项 Transmission Share: 。这样一来，当 Transmission 完成下载，就不需要手动将媒体文件移动至 PLEX 数据文件夹中。\nPLEX AniDB 插件PLEX 自带的刮削器大多是欧美电影 &#x2F; 电视节目的资料，较少包含动画的信息。这个问题可以通过安装第三方插件解决。这里推荐 Hama.bundle，安装方式参考 README - Installation https://github.com/ZeroQI/Hama.bundle#installation ，这里不赘述。\n\n\n“Scanners”         “Scanners&#x2F;Series” folder needs creating. Absolute series Scanner.py” goes inside.   https://raw.githubusercontent.com/ZeroQI/Absolute-Series-Scanner/master/Scanners/Series/Absolute%20Series%20Scanner.py\n“Plug-ins”         https://github.com/ZeroQI/Hama.bundle &gt; “Clone or download &gt; Download Zip. Copy Hama.bundle-master.zip\\Hama.bundle-master in plug-ins folders but rename to “Hama.bundle” (remove -master)\n\n\n\nhttps://github.com/ZeroQI/Hama.bundle\nhttps://github.com/ZeroQI/Absolute-Series-Scanner/\n\nPi-Hole\n\n这是一个通过污染 DNS 解析来封锁广告的工具，也可以直接在官方的 Apps 中下载到。配置流程按照官方提供的模板微调配置即可。建议通过网桥接入，自定义一个 IP 地址。注意 Docker 设置的 IP 地址要和模板后半部分设置的 ServerIP 相同。面板的密码 WEBPASSWORD 可以随便写一个，在程序运行后，进入 Docker 命令行，运行命令 pi-hole password 重置密码。\n在后期修改配置的过程中，我改动了 Pi-Hole 的 IP 地址导致面板无法正常访问。此时也可以通过进入 Docker 的 Pi-Hole 命令行，运行 sudo pihole -a -p 重新配置程序。\n\n\n如果有想使用域名访问 Pi-Hole 的需求，可以参考这篇文章。我还没有测试，不过看起来 it works。\n\nhttps://discourse.pi-hole.net/t/how-to-enable-remote-internet-access-for-pi-hole-web-dashboard/7025\n\n其他关于 Nginx 和 V2Ray 的安装，主要是用于改善我个人体验和远程访问内网需求的。这些软件安装和其他平台无异。关于用法和安装过程，我会在之后的文章介绍。\nReference\nhttps://www.reddit.com/r/unRAID/comments/7sf0hg/use_your_unraid_box_as_an_ad_blocking_dns_server/\nhttps://forums.plex.tv/t/how-do-you-install-the-official-plex-for-unraid/206596\nhttps://support.plex.tv/articles/200264746-quick-start-step-by-step-guides/\nhttps://support.plex.tv/articles/201373823-nas-devices-and-limitations/\nhttps://handlers.sans.edu/gbruneau/pihole.htm\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/06/home-network-3-hack-synology-and-more/ Written with Passion and Hope\n","categories":["Network","NAS"],"tags":["NAS"]},{"title":"家庭网络管理(4): 单网口安装 OpenWRT 实现单臂软路由","url":"//blog/2020/06/home-network-4-soft-router-at-unraid/","content":"软路由为了进一步发挥 NAS 的作用，我决定在它上面部署一个软路由。因为软路由可以更好地管理流量，部署科学上网，特别是在主路由小米 AC2100 MT7621 方案性能有限的情况下，x86 架构的软路由能够进行的加解密运算能力远远超过主路由。我的 NAS 只有板载千兆单网口，实现起来没有多网口路由器那么优雅，好在还是能够用单臂路由的方法实现软路由。\n网络拓扑规划设备一览在规划网络之前，需要先清点一下我有哪些设备，这些设备分别有哪些上网的需求。\n路由器，我有\n\n一台联通的光猫\n一台小米 AC2100\n一台小米 R3\n一台老旧的小米 3C（大概就不用了）\n\n终端设备我有\n\n若干 PC\n若干手机\n\nIoT 设备我有\n\n遍布房间的传感器\n智能插座\n开关\n\n因此，我打算将光猫作为拨号终端；小米 AC2100 作为主路由，保留小米原有系统（后来我把它刷成 OpenWRT 了）；小米 R3 无线桥接 AC2100，作为 IoT 的接入点（因为位置的原因，AC 2100 摆放在光猫边上，2.4G 没有办法很好地覆盖整个屋子，所以要用 R3 中继）。光猫的无线功能关闭，避免影响其他网络的速度。\n拓扑规划NAS 使用网线连接小米 AC2100。UNRAID 系统通过网桥（一般为 br0）让虚拟机和 Docker 上网，可以理解为一个二层交换机。我简单做个一个拓扑图，解释我打算规划的拓扑结构。\n\n\n我的规划如图，光猫负责拨号，小米 AC2100 作为主路由处理流量，R3 通过无线中继服务 IoT 设备，NAS 虚拟化各种服务和软路由提供功能。其中，主路由的 IP 地址为 192.168.31.1，软路由的 IP 地址为 192.168.31.2。为了让所有流量在转发至公网时经过软路由处理，由 AC2100、R3 所有通过 AP 接入的设备和有线接入的设备都要以 192.168.31.2（软路由） 作为网关，而软路由要以 192.168.31.1（主路由） 作为网关。为了实现这个功能，整个 LAN 中只能由软路由一个 DHCP 服务端。即，让新加入的设备自动以软路由作为网关。\n单臂软路由配置选择系统\n\n首先下载 OpenWRT 系统安装文件。进入 https://downloads.openwrt.org/releases/ 选择对应版本和对应系统架构。由于我是 x86，所以我下载了 x86 generic 版本（openwrt-19.07.3-x86-generic-combined-ext4.img）https://downloads.openwrt.org/releases/19.07.3/targets/x86/generic/。将其解压，复制到 NAS 上任意位置。由于该镜像已经打包好，可以作为硬盘使用，所以不用额外预留其他空间用于存放文件。\n\n\n部署虚拟机\n\n添加 VM - Linux，直通 CPU 建议全选上，毕竟处理网络流量比较耗费算力。BIOS 改为 SeaBIOS，主硬盘 Manual，指向 OpenWRT 镜像位置，其他保持默认即可。\n\n\n创建虚拟机并自动运行。进入 VNC，先用 passwd 命令修改默认密码，然后进入 vi /etc/config/network 修改为预定的内网 IP 地址。保存配置文件， /etc/init.d/network restart 重启网络配置。浏览器访问内网 IP 地址，一切顺利的话，机器就能启动了。由于我是演示，使用 192.168.31.3 作为 IP。\n如果 VNC 中观察到 Switching to clocksource tsc 或者其他错误导致按回车也没反应，可以考虑更换其他架构的镜像。\n初期配置\n\n安装完路由系统后，首先需要对系统进行一些初期配置。浏览器进入设置好的 IP，Network - Interfaces - Edit LAN，增加 gateway 为主路由（在我这是小米 AC2100）的 IP 地址，IPv4 broadcast 保持默认，DNS 随意设置（但是一定要设置）。切换到 DHCP Server 选项卡，将 Advanced Settings 的 Force 勾选。\nSSH 登录路由，执行 sed -i &#39;s_downloads.openwrt.org_mirrors.tuna.tsinghua.edu.cn/openwrt_&#39; /etc/opkg/distfeeds.conf 更换国内 OpenWRT 源，更新软件包 opkg update ，执行  opkg install luci-i18n-base-zh-cn 安装中文 Web 界面。如果安装过程无法正常进行，请重新检查联网设置。\n测试联网情况\n\n为了测试软路由工作情况，我们可以手工设置本地网关，修改网关至软路由对应的 IP 地址，查看连接情况。如果一切正常，就可以关掉主路由上的 DHCP 服务，让软路由提供 DHCP 服务了。一般来说，只要初期配置中路由命令行能够正常更新软件包，这一步也不会有太多问题。\n在软路由提供 DHCP 服务后，新接入的设备和需要更新 DHCP 租约的设备都会以软路由作为网关访问外网。从路径上看，就是 终端 - 主路由 - 软路由（处理流量） - 主路由 - WAN。\n性能测试延迟我使用高精度延迟测试工具 hrping 进行测试，这个工具可以从 http://www.cfos.de/en/download/start-download.htm?file=/hrping-v507.zip 下载。我将一台电脑连接至联通光猫 192.168.1.1 ，关闭防火墙，进行长时间 ping 测试。\n软路由作为网关\ntracert\n\n\nping\n\n\n主路由作为网关\nping\n\n\n可以看到，使用软路由作为路由，仅增加了 0.5ms 的延迟，稳定性也没有较大的变化。相对于软路由的好处来说，这个延迟增加是可以接受的。\n带宽和功耗TCP我使用 iperf3 作为带宽测试工具，测试终端间、终端到软路由、软路由和 NAS 之间的带宽和功耗。以下简单列出一些测试结果\nAC2100 的网口为千兆\nNAS 静止功耗29.59W\nPC（软路由网关）- PC（光猫 LAN）\n\n由于联通的光猫只有一个千兆口，剩下全是百兆口，所以实际测速只有百兆。此时的 NAS 功耗为 37 - 40W\nPC（软路由网关） - 软路由800Mb 49W\nPC（软路由网关） - NAS930Mb 53W\n软路由 - NAS （网桥内网）6.5-7.5Gb 58W\n软路由 V2Ray 加密带宽和功耗我通过 OpenClash 挂 V2Ray 的梯子，远程访问我的服务器，测试经过 V2Ray 加密之后的功耗。实际测试结果为\n100Mb 37W260Mb 43W\nTCP with V2Ray上述 V2Ray 的测试让我突发奇想，打算测试一下 NAS CPU 在满载 V2Ray 时的负载，看看这块 CPU 对 V2Ray 流量的处理能力。为了让 iperf3 通过代理，我简单编写了一个（自攻自受）的反向代理，让 V2Ray 在此期间进行一轮加密和一轮解密。iperf3 的流量穿过一层 V2Ray 的隧道，在网桥内网中测试。\n软路由 - NAS （没有 V2Ray）\n6.5-7.5Gb 58W\n\n\n\n软路由 - NAS （V2Ray 一次加密，一次解密 + 反向代理）\n2.5-3Gb 58W\n\n\n\n结果表明这块 CPU 的性能和功耗还是非常棒的，在满载的情况下可以同时处理 2.5Gb 带宽以上的 V2Ray 流量（实际情况应该和这个不同，实际情况中不需要一次加密和一次解密，但是需要考虑其他运算，例如 ws，tls），应付日常使用绰绰有余。\n展望一切看起来都很美好，但是也有美中不足的地方。UNRAID 运行的 Docker 无法以软路由作为网关，即 UNRAID Docker 的流量无法经过软路由。目前我并不知道为什么会这样，可能是由于启动顺序不同导致的问题。因为如果要修改 UNRAID 的默认网关，需要停止整个阵列。一旦阵列停止，Docker 和 VM（包括软路由）也需要停止。如果修改完设置，启动阵列时，Docker 会因为没有 DHCP 服务器而无法分配到 IP 地址（一般来说 VM 启动优先级更慢）。我正在继续寻找解决方案。\n由于 CPU 性能足够，我打算下一步上万兆网卡和万兆交换机，让内网进一步加速。同时，我打算直接在 NAS 上安装万兆网卡。这块主板还有一个 PCIE x16 插槽，可以插一张多口万兆网卡实现软路由硬件直通，进一步提高效率。\nReferences\nhttps://openwrt.org/toh/views/toh_fwdownload\nhttp://www.cfos.de/en/ping/ping.htm\nhttps://mirrors.tuna.tsinghua.edu.cn/help/openwrt/\nhttps://post.smzdm.com/p/az502200/\nhttps://www.jianshu.com/p/da01ce070688\nhttps://post.smzdm.com/p/ax027g83/\nhttps://toutyrater.github.io/app/reverse.html\nhttps://iperf.fr/iperf-doc.php\n感谢拓扑绘画工具 https://www.processon.com/ 尽管使用中还遇上很多 BUG\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/06/home-network-4-soft-router-at-unraid/ Written with Passion and Hope\n","categories":["Network","NAS","Router"],"tags":["NAS","Router"]},{"title":"家庭网络管理(2): 部署和使用 UNRAID","url":"//blog/2020/06/home-network-2-enjoy-with-UNRAID/","content":"选择 UNRAID在组装完 NAS 之后，下一步就是考虑如何将 NAS 的硬件性能高效利用。经过对群晖、UNRAID、FreeNAS 等 NAS 操作系统的调研后，我决定使用 UNRAID 作为我的 NAS OS。一是因为 UNRAID 对 Docker 和 VM 的支持较好，有庞大的社区支持；二是因为 UNRAID 的阵列功能很好用，自带的软 RAID 功能可以只用一张 Parity Check 盘完成其他所有盘的校验；三是因为 UNRAID 的 Web UI （相对）比较符合我的审美。话不多说，立刻开始破解版 UNRAID 安装。\n安装UNRAID 的官网是 https://unraid.net/ ，价格表在 https://unraid.net/pricing 。从价格上看，Basic 订阅只要 60 刀，还是比较合适的。由于我第一次使用这个操作系统，所以先从破解版入手。\n\n\n安装 UNRAID 需要准备一个 U 盘作为启动盘和系统盘，并且之后整个系统就安装在这块 U 盘中。这个 U 盘在以后使用 UNRAID 时，需要一直插在 USB 口上。不能使用 SD 卡 + 读卡器，因为可能获取不到正确的设备 ID。 UNRAID 系统根据闪存的设备 ID 分发密钥。在安装之前，需要将闪存格式化成 FAT32 格式。如果 Windows 不支持格式化为 FAT32 ，可以使用 Windows GUI version of fat32format 将闪存格式化为 FAT32 格式。切记，在进行格式化操作之前，务必做好数据备份。\n截至目前，最新的破解版是 UNRAID 6.8.2 ，下载地址在 http://www.hopol.cn/2020/06/1675/ 。这个安装包自带密钥计算工具，可以使用 https://koolshare.cn/thread-181253-1-1.html 提供的一键安装工具进行安装；或者按照破解版下载页面的安装方法设置 UNRAID U 盘 启动，然后再回到 Windows 进行破解密钥生成。\n初期配置点亮先将显示器接上运行 UNRAID 的机器。在开机启动时，狂按 Del 进入 BIOS 配置页面，修改默认主板设置。我先修改默认启动引导至装有 UNRAID 系统的 U 盘上，然后打开了 CPU 的虚拟化功能（VT-d），F10 保存重启。\n启动选项选择 Unraid OS (Headless) 。如果正常启动，在命令行可以看到 UNRAID 被分配到的 IP 地址。通过 Web 访问即可。默认账户为 root ，无密码。\n\n\n配置网络\n\n第一步需要配置网络，保证机器使用固定的 IP 地址。在阵列停止的情况下，进入 SETTINGS - Network Settings 配置网络。为了让 IP 不发生变化，建议给 UNRAID 设置一个静态 IP，网关指向当前主路由。由于目前我的网络没有运行 IPv6 服务，故没有设置 v6 地址。\n配置阵列UNRAID 支持软 RAID 0 数据盘（data divices）、校验盘（parity devices）和缓存盘（cache devices）。其中，数据盘和校验盘建议使用机械硬盘，并且校验盘的容量必须大于任意一个数据盘。缓存盘可以使用固态硬盘。使用缓存盘将会极大改善用户体验。所有新安装的硬盘都需要先格式化再运行，格式化选项可以在主页看到。\n\n\n我有两块 HDD 和一块 SSD ，因此我将 SSD 作为缓存盘，HDD 作为数据盘。我暂时不打算上校验盘。\n共享文件夹的缓存方式\n\n我们可以通过 User Share 面板创建内网共享目录，创建出的共享目录可以被 Windows 发现。在系统中挂载了缓存盘的情况下， UNRAID 有四种共享文件夹的缓存策略。\n\nYes 只要缓存上的可用空间大于设置的最小可用空间，总是将新文件写入缓存，反之就直接把文件写入数据盘。mover* 运行时，如果文件没有被占用，它就会尝试回写。哪个阵列驱动器将得到文件是由分配方法和共享的Split级别设置的组合控制的。这是一种传统的、缓存工作的方式。\nNo 直接将数据写入数据盘。mover* 运行时，即使缓存中有理论上属于对应共享文件夹的数据，也不会写回。这种共享方式不使用缓存。\nOnly 直接向缓存中写入新文件。如果缓存上的空闲空间低于设置的最小剩余空间，将抛出空间不足错误并写入失败。mover* 运行时，即使数据盘中有逻辑上属于这个共享的文件，它也不会对这个共享的文件采取任何行动。这意味着这个共享文件夹只存放于缓存，并不存放于数据盘。\nPrefer 只要缓存上的可用空间大于设置的最小可用空间，总是将新文件写入缓存，反之就直接把文件写入数据盘。mover* 运行时，只要缓存上的可用空间大于设置的缓存最小可用空间，它就会尝试将数据盘上对应的共享文件夹移回缓存。这样做的好处是让 VM &#x2F; Docker 和存放于 appdata 中的配置文件能够更快地被读取和修改，这将大大加速配置文件的读写速度。特别是在需要校验盘进行奇偶校验计算时。即使暂时没有缓存，这个设置也适用。这种策略比较特殊。它意味着 UNRAID 在这个文件共享下更__“倾向于”__使用缓存，但是没有缓存也不会影响实际使用。\n\n* 此处的 Mover 指的是根据缓存设置，在数据盘和缓存盘之间同步数据的工具，由 UNRAID 默认提供。\n缓存性能测试在并不算太好的 HDD 和 SSD 组成的阵列下，我进行了一系列缓存性能的测试。由于电脑和路由器只有千兆网口，速度并不能达到原有 SSD 的读写水平。\n缓存策略 Yes\n\n\n缓存策略 No\n\n\n可以看到，虽然读写速度受限于网口带宽上限，但是使用了缓存的共享目录读写性能仍有一定的提升。我打算在之后升级万兆网卡，这个瓶颈应该可以得到解决。\n在机器上直接对硬盘做读写速度测试，可以得到以下结果。从理论上说，如果能够升级万兆内网，读写性能应该可以到达理论速度。下图的测试中，sdb，sdc 是机械硬盘，sdd 是固态硬盘。\n\n\n常用插件安装官方 Community Apps 支持UNRAID 社区已经发展到一定的规模了，社区中不少人会制作长期适用于 UNRAID 的官方 Apps。他们有的是封装好 UNRAID 模板的 Docker 容器，有的可以给 UNRAID 增加功能。进入 Plugins - Install Plugin，输入 https://raw.githubusercontent.com/Squidly271/community.applications/master/plugins/community.applications.plg 安装。安装完成后刷新 Web 页面，就可以看到上方菜单多出了一个 Apps 栏。在 Apps 中，就能安装很多 NAS 常用的工具了。\nNerd Tools\n\n可以使用官方 Apps 后，首先我会考虑安装的是 NerdPack GUI 。这是一个可以安装额外软件（大多是 CLI 软件）的工具箱，可以理解为一个简化了的 apt 源 ，源里的大部分程序包是 Linux 控制台操作中常用的那部分。点击 Plugins - Nerd Tools ，点击左下角 Check Update ，就可以选择安装对应软件包。这对日常使用，特别是习惯于操作常见 Linux 发行版的用户来说非常方便。\n\n\nUnassigned Devices\n\n这是一个支持挂载其他 NFS &#x2F; SMB 虚拟盘作为数据盘的工具，在 APPS 里可以安装。具体使用场景虽然我还没用上，但是可以考虑一个虚拟场景：\n我有一块群晖数据盘，不想搬迁数据。我可以通过直通将数据盘挂载在 UNRAID 虚拟出来的黑群晖上，然后黑群晖开 SMB &#x2F; NFS 给 UNRAID 使用。或者局域网中的其他 BT 下载机的下载盘，挂载在 UNRAID 下，用于 PLEX 或者 Jellyfin 播放。\nDynamix S3 Sleep\n\n这是一个可以自动判断硬盘是否在工作，在特定时间让 UNRAID 服务器休眠的工具，在 APPS 里可以安装。但是由于后来我安装了软路由，所以就没有继续开启睡眠功能。有需要的时候这个工具还是很方便的，可以定时关机，降低功耗和噪音。\n其他调整更换 Docker 镜像由于 UNRAID 对 Docker 生态有较大的依赖，很多 Docker 应用都可以和 UNRAID 协同工作。需要更换 Docker 国内镜像，以达到较好的使用体验。这里推荐阿里云的 Docker 镜像加速器，访问 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 就可以申请。拿到自己的 Docker 镜像加速器地址后，在 Web Terminal 中执行以下命令对 Docker 源进行更换。\nsudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [&quot;https://&#123;YOUR_DOCKER_MIRROR_ID&#125;.mirror.aliyuncs.com&quot;]&#125;EOF\n\n为了在每次重启时自动应用配置，建议将其写入 \\flash\\config\\go 中。其中 \\flash 指装有 UNRAID 的 U 盘根目录。\n允许 Docker 虚拟机访问宿主机由于 Docker 的网络隔离，有时候我想进行一些特殊的操作就比较困难。比如我将所有的服务用一个 NginX 反代，但是使用 Host 或者 Bridge 方式暴露端口在宿主机上的服务就无法被反代。因此，我需要手工修改 UNRAID 的 Docker 设置。进入 Settings - Docker ，点击右上方的 Basic View ，切换成 Advanced View ，然后修改对应选项即可。这么做存在一定风险。我认真检查防火墙，保证外网的访问能够安全地被处理后，才进行这个操作。\n\n\n\n\n以上，我的 UNRAID 就做好作为一台 NAS 进行工作的准备了。下一章我会继续介绍我安装的应用程序和配置，合理发挥 NAS 的功能。\nReferences\nhttps://unraid.net/\nhttps://forums.unraid.net/topic/38582-plug-in-community-applications/\nhttps://wiki.unraid.net/UnRAID_6/Getting_Started\nhttps://wiki.unraid.net/UnRAID_6/Storage_Management\nhttps://wiki.unraid.net/Check_Harddrive_Speed\nhttps://forums.unraid.net/topic/38582-plug-in-community-applications/\nhttps://forums.unraid.net/topic/35866-unraid-6-nerdpack-cli-tools-iftop-iotop-screen-kbd-etc/\nhttps://forums.unraid.net/topic/77390-cannot-access-dockers-using-custombr0/\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/06/home-network-2-enjoy-with-UNRAID/ Written with Passion and Hope\n","categories":["Network","NAS"],"tags":["NAS"]},{"title":"优雅地管理内网集群","url":"//blog/2020/04/intranet-cluster-management/","content":"本文主要介绍了我在对内网集群进行管理的时候遇上的和解决的问题，包括统一控制，装机脚本，堡垒机，内网穿透等一系列问题。\n突然有了内网集群由于科研需要，实验室购置和申请了大量服务器和显卡，并申请了从超算中虚拟出来的计算节点。如何将他们统一管理成了一个很大的问题。同时，由于机器都托管在数据中心，需要 VPN 进行访问。数据中心提供的 VPN 是上古的 L2TP/IPsec with Pre-shared key，还要在注册表写信息允许弱加密才能在 Windows 10 上使用， Mac &#x2F; Linux 电脑完全无法连上这批机器。同时，VPN 还严格限制同时只能一个人使用，一旦使用的人数增加，需要多次申请不同的账号，管理起来十分麻烦。为了解决一系列的问题，我开始探索方案。\n统一管理：堡垒机选择 JumpServer俺找了有相关经验的好朋友奶鱼 ，他推荐了一套堡垒机方案 JumpServer。这是一套开源的堡垒机程序，支持身份认证，账号管理，资产授权，操作审计等管理功能，也支持批量部署，Web Terminal 等通过 Web 操作机器的功能。整体上看，非常符合我管理内网三十多台机器的需求。\n\n开源仓库一览\n\nJumpServer https://github.com/jumpserver/jumpserver\nKoko (SSH, WS server) https://github.com/jumpserver/koko\nLuna (Web terminal) https://github.com/jumpserver/luna\n\n\n前期准备要想部署一个能够管理所有服务器的堡垒机的平台，就需要给每个节点部署一个带 sudo 权限的用户。这个用户最好在不同机器上是统一的。现有的服务器用户虽然也有部分统一，但使用一个独立的用户管理所有服务器更符合直觉。我先创建一个统一的用户 touko ，并赋予其 sudo NOPASSWD 权限。\necho &quot;SUDO_PASSWORD&quot; | sudo -S useradd -m touko -s /bin/bashecho &quot;touko ALL=(ALL:ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/touko\n\n为了安全，我创建了一组 ECDSA 算法的密钥对，然后将它的公钥导入 authorized_keys 作为登陆密钥。\nssh-keygen -t ecdsa -C &#x27;jumpserver&#x27;cd ~/.sshcat id_ecdsa.pub &gt;&gt; authorized_keys\n\n然后，将创建的 ECDSA 私钥 ~/.ssh/id_ecdsa 备份并删除。在其他的机器上，编写脚本，拷贝私钥至脚本中，就能一键完成部署（顺便退出命令行）。\n\n\necho &quot;SUDO_PASSWORD&quot; | sudo -S useradd -m touko -s /bin/bashecho &quot;touko ALL=(ALL:ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/toukosudo su toukomkdir ~/.sshecho &quot;ecdsa-sha2-nistp256 YOUR_ECDSA_KEY&quot; &gt;&gt; ~/.ssh/authorized_keysecho &quot;PubkeyAuthentication yes&quot; | sudo tee -a /etc/ssh/sshd_configecho &quot;AuthorizedKeysFile      .ssh/authorized_keys .ssh/authorized_keys2&quot; | sudo tee -a /etc/ssh/sshd_configsudo systemctl restart sshexitexitexit\n\n这样一来，就可以用同样的私钥登录每台服务器。对需要被管理的服务器来说，只需登上去运行一次脚本，就能完成加入堡垒机的前期配置。\nJumpServer 的安装JumpServer 的安装有详细的文档，具体可以在此查看。由于规模不大，我直接使用了官方提供的 极速安装 指令。该指令只适用于 CentOS 7.x。该指令会安装 JumpServer 主程序，并在 Docker 下运行 koko 和 luna。koko 和 luna 有时候会假死（表现为 Web Terminal 无响应），需要手工重启 JumpServer。\ncd /optyum -y install wget gitgit clone --depth=1 https://github.com/jumpserver/setuptools.gitcd setuptoolscp config_example.conf config.confvi config.conf# Install./jmsctl.sh install\n\n手工重启 JumpServersudo docker stop jms_kokosudo docker stop jms_guacamolesudo systemctl stop jms_coresudo systemctl start jms_core sudo docker start jms_kokosudo docker start jms_guacamole\n\n在使用极速安装之前，我一度偷懒使用 Docker 部署。但实际测试起来，用 Docker 运行的堡垒机非常不稳定，特别是 Web Terminal 经常跑着跑着就断线。最后我还是额外申请了一台 CentOS 的服务器（已有服务器全是 Ubuntu），作为集群的堡垒机。\nJumpServer 的使用JumpServer 的设计非常用户友好，基本不需要额外学习就能很快上手。下图是我添加完所有节点后的截图，可以一目了然的知道当前系统有多少机器，机器如何分组规划。\n\n\nV2Ray 内网穿透堡垒机的问题解决后，下一个要解决的问题是内网穿透。我只打算将堡垒机作为用户管理和应急使用的 Web Terminal，一是由于就算是极速安装的 JumpServer ，依然会出现 Web Terminal 假死的情况。目测是由于其他两个运行于 Docker 下的程序（koko，Luna）中某个程序出问题导致的。\n这时候就需要内网穿透工具。经过一系列的调研和尝试，最终选择了我熟悉的 V2Ray，提供 JumpServer Web UI 和用户直接访问机器的穿透服务。\n原理内网穿透依赖的是 V2Ray 中的反向代理功能，该功能在 V2Ray 4.0+ 后支持。我用一张图来解释 V2Ray 中的内网穿透是如何工作的。\n\n\n在内网穿透用的 V2Ray 程序中，节点可以被分为两类，一类叫 bridge，另一类叫 portal 。这两类属性定义于 V2Ray 配置文件中 ReverseObject 项。bridge 作为内网的一方，会主动向 portal 方发起连接并长期保持。bridge 和 portal 都需要手工定义路由表来实现定向的流量转发。\n建议先运行公网服务器 portal，再运行内网服务器 bridge。内网服务器会根据配置的 vmess 连接方式，连接公网服务器对应端口，并接收其所有流量。这就完成了内网穿透的第一步，保持一条从内网到公网的隧道（tunnel），让内网服务器可以主动接受其他连接。为了实现反向代理流量的转发，我们还需要手工配置路由表。直接从配置文件入手。\n配置文件样例此处我直接修改来自 V2Ray 白话文教程的配置文件样例。\n内网服务器&#123;  &quot;reverse&quot;: &#123;    &quot;bridges&quot;: [&#123;      &quot;tag&quot;: &quot;bridge&quot;,      &quot;domain&quot;: &quot;same.as.others.com&quot;    &#125;]  &#125;,  &quot;outbounds&quot;: [&#123;    &quot;tag&quot;: &quot;tunnel&quot;,    &quot;protocol&quot;: &quot;vmess&quot;,    &quot;settings&quot;: &#123;  // 公网服务器 vmess 连接方式      &quot;vnext&quot;: [&#123;        &quot;address&quot;: &quot;public.touko.moe&quot;,          &quot;port&quot;: 10086,        &quot;users&quot;: [&#123;          &quot;id&quot;: &quot;b831381d-6324-4d53-ad4f-8cda48b30811&quot;,          &quot;alterId&quot;: 64        &#125;]      &#125;]    &#125;  &#125;, &#123;    &quot;protocol&quot;: &quot;freedom&quot;,    &quot;tag&quot;: &quot;lan&quot;  &#125;],  &quot;routing&quot;: &#123;    &quot;rules&quot;: [&#123;  // Rule 1      &quot;type&quot;: &quot;field&quot;,      &quot;inboundTag&quot;: [&quot;bridge&quot;],      &quot;domain&quot;: [&quot;full:same.as.others.com&quot;],      &quot;outboundTag&quot;: &quot;tunnel&quot;    &#125;, &#123;  // Rule 2      &quot;type&quot;: &quot;field&quot;,      &quot;inboundTag&quot;: [&quot;bridge&quot;],      &quot;outboundTag&quot;: &quot;lan&quot;    &#125;]  &#125;&#125;\n\nRule 1 定义 bridge 应该主动连接 tag 为 tunnel 的公网服务器，并附带识别域名 same.as.others.com 。这个域名需要保证在 bridge 、 portal 和其路由表中都是一样的（共四处）。Rule 2 定义当 bridge 收到来自 tunnel 的反向连接后，转发至 tag 为 lan 的 outbound，在本配置文件中是 freedom，也就是直接从内网服务器的网卡发出请求。这样一来，就能直接访问内网资源。\n公网服务器&#123;  &quot;reverse&quot;: &#123;    &quot;portals&quot;: [&#123;      &quot;tag&quot;: &quot;portal&quot;,      &quot;domain&quot;: &quot;same.as.others.com&quot;    &#125;]  &#125;,  &quot;inbounds&quot;: [&#123;    &quot;tag&quot;: &quot;web&quot;,    &quot;port&quot;: 80,    &quot;protocol&quot;: &quot;dokodemo-door&quot;,    &quot;settings&quot;: &#123; // 内网服务器需要暴露的 IP、端口和协议      &quot;address&quot;: &quot;127.0.0.1&quot;,        &quot;port&quot;: 8080,      &quot;network&quot;: &quot;tcp&quot;      &#125;  &#125;, &#123;    &quot;tag&quot;: &quot;tunnel&quot;,    &quot;port&quot;: 10086,    &quot;protocol&quot;: &quot;vmess&quot;,    &quot;settings&quot;: &#123;      &quot;clients&quot;: [&#123;        &quot;id&quot;: &quot;b831381d-6324-4d53-ad4f-8cda48b30811&quot;,        &quot;alterId&quot;: 64      &#125;]    &#125;  &#125;],  &quot;routing&quot;: &#123;    &quot;rules&quot;: [&#123;  // Rule 3      &quot;type&quot;: &quot;field&quot;,      &quot;inboundTag&quot;: [&quot;tunnel&quot;],      &quot;domain&quot;: [&quot;full:same.as.others.com&quot;],      &quot;outboundTag&quot;: &quot;portal&quot;    &#125;, &#123;  // Rule 4      &quot;type&quot;: &quot;field&quot;,      &quot;inboundTag&quot;: [&quot;web&quot;],      &quot;outboundTag&quot;: &quot;portal&quot;    &#125;]  &#125;&#125;\n\nRule 3 定义当流量从 tunnel 进入时，转发至 portal。同样的，Rule 4 定义当流量从 web 进入时，转发至 portal。这两个路由规则一一对应图中的 Method B 和 Method A。如果使用 Method B 访问，；流量的出口就是内网服务器的 LAN，就实现了和挂 VPN 相同的效果。如果使用 Method A 访问，就会根据 tag 为 web 的 dokodemo-door setting 中的配置访问对应的端口。这分别对应内网穿透暴露端口的功能和挂 VPN 访问其他内网资源的功能。\n值得一提的是，这里复用了内网穿透中接收内网服务器反向连接的隧道和用户访问 Method B 的隧道。在实际使用过程中，可以创建更多的 vmess inbound 并定义路由表来实现 vmess uuid 的分离。\n结语通过内网穿透的配置，我将堡垒机的管理面板暴露至公网，并用 NginX 反代，成功实现了不使用 VPN 管理内网集群的方法，并打包了一组 V2Ray 程序代替 VPN，使得需要进行计算任务的同学可以直接使用本地的 HTTP 代理或直接转发服务器 SSH 端口访问集群，不再需要通过 VPN。这方便了实验室硬件资源的管理，并能够更好控制由于人员流动带来的权限吊销问题。在实际运行的一个季度中，整套系统稳定性较高，V2Ray 的反向代理也没有出现崩溃的情况。除了上传文件的速度受限于公网服务器的带宽（阿里云，只有突发 5M），其他并没有什么不好的体验。JumpServer 也非常好用，特别是可以配置 2FA 严格控制用户的登录行为，甚至可以重播用户在 Web Terminal 进行的操作。在服务器宕机时，能够更方便地寻找原因。\n这套系统还在不断进行改进，今后会根据实际情况调整。\nReferences\nhttps://jumpserver.readthedocs.io/zh/master/setup_by_fast.html\nhttps://superuser.com/questions/67765/sudo-with-password-in-one-command-line\nhttps://blog.csdn.net/hejinjing_tom_com/article/details/7767127\nhttps://guide.v2fly.org/app/reverse.html\nhttps://www.v2ray.com/chapter_02/reverse.html\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/04/intranet-cluster-management/ Written with Passion and Hope\n","categories":["Linux","Network"],"tags":["V2Ray","NAT Traverse"]},{"title":"在树莓派上运行 OpenWrt 作为软路由","url":"//blog/2020/07/raspi-as-router/","content":"前言之前我用 NAS 虚拟化了一台 OpenWrt 用作软路由。随着使用频率的增加，我越发离不开它带来的优秀的上网体验。但是用虚拟化的软路由总觉得有些不舒适，比如万一 NAS 掉线（虽然暂时没出现过），内网就会瞬间瘫痪。同时，用这种方法虚拟出来的软路由和小米路由自带的 Mesh 功能有冲突，小米路由无法获取 Mesh 节点的拓扑图，也没有办法进行完整的设备管理（识别不出设备）。\n解决方案是在小米路由器的上级部署软路由，将原本放在小米路由器内网的软路由移动到光猫内网。我先用树莓派 4 作为软路由进行测试，另一块 1037U 主板在路上。光猫只有一个千兆网口，但是恰好前些时间我买了台网件的 GS105 ，得以在光猫的内网中接入更多千兆设备。\n\n\n这是一个五口非网管型交换机，自适应 10&#x2F;100&#x2F;1000M，带指示灯，发热量不大，价格百元内。这样一来，网络拓扑就可以进行下面的改动\n\n\n将内网的 DHCP 分发任务交给小米路由，允许小米路由 app 完全发挥功能，带来更严格的 WiFi 接入控制和 Mesh 组网体验。另一方面，改变静态 IP 配置将小米路由的网关设置为软路由地址，让其流量通过作为软单臂路由的树莓派，达到科学上网等目的。\n树莓派组装\n\n树莓派和外壳都是我从咸鱼购入的。我前期测试过树莓派 3B 的性能，的确不太行，因此这次买了树莓派 4 做软路由，外加一个红色的外壳、一个噪音超大的蓝光散热风扇和几块散热片，总价不到 230。\n\n\n先上散热片。考虑后期可能会更换其他散热，我没有直接把散热片粘在机器上，而是先粘上一层导热垫，用导热垫自身的黏性固定散热片。其中两块大的散热垫分别对应 CPU 和内存，还有一块不知道贴哪，就随便贴了个图一乐。\n然后将外壳风扇的 5V 供电安装在树莓派 5V 针脚和接地上。根据 https://www.raspberrypi.org/documentation/usage/gpio/ 的定义，将红色和黑色两条线分别接在 4、6 号针脚即可。\n\n\n实际使用中，风扇转速太大，噪音感人，光害也很令人头疼。我打算目前不接这个风扇。如果未来热量堆积严重，我会考虑更换其他的静音风扇。\nOpenWrt 安装和配置烧录 OpenWrt 系统从 OpenWrt 的官网可以检查当前 OpenWrt 在树莓派各版本系统镜像的发行情况。在树莓派 4 上，官方目前没有正式发行的 Release 版本，只有 Snapshot 版本的 OpenWrt 镜像。Snapshot 版本的镜像不带 LuCi 界面，因此，在安装过程中可能会遇到一些麻烦。\n\n\n\nThe Raspberry Pi is supported in the brcm2708 target.Subtargets are bcm2708 for Raspberry Pi 1, bcm2709 for the Raspberry Pi 2, bcm2710 for the Raspberry Pi 3, bcm2711 for the Raspberry Pi 4.\n\n根据官方说明，树莓派 4 选择的对应 subtarget 为 bcm2711，因此我们可以顺藤摸瓜到 https://downloads.openwrt.org/snapshots/targets/bcm27xx/bcm2711/ 下载对应的系统镜像 rpi-4-ext4-factory.img.gz。如果网不好，也可以到我们的老朋友 TUNA 镜像的另一个分身 - 北外开源软件镜像站 https://mirrors.bfsu.edu.cn/openwrt/snapshots/targets/bcm27xx/bcm2711/ 下载。为什么不直接从 TUNA 下载呢？因为截止目前，所有对 https://mirrors.tuna.tsinghua.edu.cn/openwrt/snapshots/ 的访问都会被重定向至 https://mirrors.bfsu.edu.cn/openwrt/snapshots/ 。\n插上读卡器，打开 Raspberry Pi Imager ，烧录系统。\n\n\nOpenWrt 网络初始配置把 SD 卡插进树莓派，通电开机。第一次开机时，先将电脑直接和树莓派的网口相连。因为 OpenWrt 默认 IP 为 192.168.1.1 并默认开启 DHCP ，只要将电脑接入，IP 就会自动分配。为了避免可能的路由冲突，可以禁用其他网卡。\n\n\n由于树莓派是单网口的，如果这个网口用于连接 PC，就没有办法上网了。此时需要通过 SSH 连接树莓派进行网络配置。Snapshot 的 OpenWrt 系统默认无密码，在使用 SSH 客户端连接时，需要选择 none 的连接方式。\n\n\n先运行命令 passwd 设置 root 密码。密码设置完成后，断开并重连 SSH 测试密码设置是否正确。\n然后修改网络配置。我准备接入网段为 192.168.1.1&#x2F;24 的局域网，网关是 192.168.1.1，为了避免 IP 冲突，让树莓派可以上网，需要修改网络配置。\nvi /etc/config/network\n\n可以看到默认的 lan 网络设置为\nconfig interface &#x27;lan&#x27;        option type &#x27;bridge&#x27;        option ifname &#x27;eth0&#x27;        option proto &#x27;static&#x27;        option ipaddr &#x27;192.168.1.1&#x27;        option netmask &#x27;255.255.255.0&#x27;        option ip6assign &#x27;60&#x27;\n\n根据需求，修改 IP 地址，增加网关地址和 DNS\nconfig interface &#x27;lan&#x27;        option type &#x27;bridge&#x27;        option ifname &#x27;eth0&#x27;        option proto &#x27;static&#x27;        option ipaddr &#x27;192.168.1.2&#x27;        option netmask &#x27;255.255.255.0&#x27;        option ip6assign &#x27;60&#x27;        option gateway &#x27;192.168.1.1&#x27;        option dns &#x27;119.29.29.29&#x27;\n\n最后运行\nservice network restart\n\n此时控制台会显示 &#39;radio0&#39; is disabled 然后无响应，因为网络接口发生了变化。\n将树莓派接回交换机，SSH 连接刚才设置的 IP 地址。如果设置无误，此时可以连上树莓派。初次连接的网络配置完成。\n安装 LuCi替换 OpenWrt 软件源为北外源\nsed -i &#x27;s_downloads.openwrt.org_mirrors.bfsu.edu.cn/openwrt_&#x27; /etc/opkg/distfeeds.conf\n\nopkg updateopkg install luci\n\n如有需要，可以安装语言包\nopkg install luci-i18n-base-zh-cn\n\n直接访问设置好的 IP 地址，就能在图形界面正常使用 OpenWrt 了。\n\n\n性能测试为了让树莓派能够胜任原有 NAS 上虚拟化的软路由的工作，我对树莓派 + Clash 的性能做了一次测试。目标代理用的是 V2 + TLS + WS。\n\n\n在不走代理的情况下，树莓派的网络吞吐和 NAS 相比在误差范围内。 在代理的情况下，由于 ARM 的性能和 x86 的 NAS 有一定差距，NAS 基本跑满了代理的带宽，而树莓派只有大约 2&#x2F;3 的性能，日常使用差别不会太明显。\nReferences\nhttps://openwrt.org/toh/raspberry_pi_foundation/raspberry_pi\nhttps://www.raspberrypi.org/documentation/usage/gpio/\nhttps://openwrt.org/docs/guide-user/luci/luci.essentials\nhttps://openwrt.org/zh-cn/doc/uci/network\nhttps://mirrors.bfsu.edu.cn/help/openwrt/\nhttps://openwrt.org/packages/pkgdata/luci-i18n-base-lang\nhttps://openwrt.org/docs/guide-user/services/nas/sftp.server\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/07/raspi-as-router/ Written with Passion and Hope\n","categories":["Network","Router","Raspi"],"tags":["Router","Raspi"]},{"title":"在树莓派上运行 Home Assistant，支持米家等 IoT 设备接入 HomeKit","url":"//blog/2020/06/run-hass-on-raspberry/","content":"前言捡了块树莓派临时被叫回学校办毕业手续，想起当年（五年前）我在淘宝买了块树莓派 2B，给家里做了个通过 HTTP 代理自动分流国内外流量的工具。那块树莓派被我带到了学校，放着吃灰三年多了。这次回学校，就顺手把这块树莓派带了回来。\n最近优化了家里的网络，大学四年也断断续续买了不少米家的 IoT 设备。米家设备能够原生支持 HomeKit 也应该只是最近的事，我手上只有一个台灯、三个智能插座能够加入 HomeKit 管理。因此，我打算把这台树莓派挂在 IoT 设备专用中继路由器下，作为一个中介，将其他无法原生支持 HomeKit 的米家设备接入 HomeKit，实现统一管理。\n为什么我不直接用米家做管理呢？一是米家的操作逻辑比较恼人；二是米家无法长期运行于后台，一些和离家、回家相关的自动化逻辑无法很好的运行。接入 HomeKit 后，这种情况会得到一定的改善。\n桥接平台的选择通过资料调查和实际测试，我最终选择了 Home Assistant 作为接入智能家居设备的平台。同类解决方案还有很多，比如同样提供智能家居接入方案的 Home Bridge，ioBroker 等。我同时尝试了 Home Bridge，但是由于米家相关插件文档模糊，接入后无法正常连接挂载在网关的设备，最终作罢。\n其实 Home Assistant 的安装也并不容易。安装过程存在各种各样的问题和 bug，我尝试了非常多种方案才成功运行。\n树莓派的初步配置SD 卡安装树莓派系统我掏出了之前留在机器上的传家宝 TF 卡 三星 EVO+ 32G，不料卡在经过几次刷写后，自动写保护了。于是我又从 NS 里掏出了从 tb 20 RMB 买的朗科 32G TF 卡。现在 TF 卡的价格真的越来越便宜了，我打算屯一堆，免得弄坏了没有替换卡。\n\n\n\n\n打开 Windows 自带的 Disk Management，将原有 SD 卡分区删除，创建一个大的 FAT32 分区。这一步也可以用树莓派官方自带的 Raspberry Pi Imager 格盘。这个软件可以在 https://www.raspberrypi.org/downloads/ 下载到，推荐用这个软件烧写系统盘。\n\n\n由于是树莓派 2B，性能一般，所以需要尽可能地压榨硬件资源。在系统选择页面 https://www.raspberrypi.org/downloads/raspberry-pi-os/ 中，我选择了不带图形界面的 Raspberry Pi OS (32-bit) Lite。下载并解压，然后用官方烧写工具烧写系统镜像。这张 TF 卡的速度意外的很快，在 USB 3.0 读卡器下，大约有80-90MB&#x2F;S 的写入速度。\n\n\n\n\n现在的树莓派官方系统是默认不开启 SSH 功能的。开启 SSH 功能的方法除了通过外接显示器在命令行输入 sudo raspi-config 外，也可以直接在烧写后的 TF 卡根目录下创建命名为 SSH 的空文件，以激活 SSH 功能。烧写完成后，由于软件会自动弹出对应设备，需要重新插拔一下读卡器，并创建对应空文件。\n我预先在 DHCP 服务器上设置了分配的静态 IP。如果不清楚树莓派被分配到的 IP，可以在路由器中 DHCP 租约下找到被分配的 IP 地址。建议要在路由器上做好 IP 分配。\n镜像源官方系统默认的账户和密码是 pi &#x2F; raspberry。登入 SSH 后，更换清华的镜像以提高 apt 更新速度。以 Debian 10（buster）为例\n\n编辑 /etc/apt/sources.list 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contrib rpideb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contrib rpi\n编辑 /etc/apt/sources.list.d/raspi.list 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ buster main ui\n\nsudo apt updatesudo apt upgrade\n\n安装 Home AssistantHome Assistant，Hass 和 Hassbian 的关系这三者之间的关系把我绕进去了，我也是在经过大量搜索才明白了他们到底是什么。这个统一管理的程序叫 Home Assistant，它曾经叫 Hass.io。 Hassbian 是一个由官方封装好的，可以开箱即用运行 Home Assistant 的系统，且仅能运行 Home Assistant 系统，命名应该是参考了 Raspbian ，并且是基于其构建的。\n安装依赖因为我们使用的是基于 Python3 运行的 Home Assistant，所以要安装 Py3 相关的依赖。同时，为了避免对运行的其他程序的影响，更方便维护，使用 python venv 作为虚拟环境运行程序。由于我在上海，所以我更换了交大的 pypi 镜像。也可以考虑清华镜像或科大镜像。\nsudo apt install -y python3 python3-dev python3-venv python3-pip libffi-dev libssl-dev autoconfsudo apt install -y libavahi-compat-libdnssd-devsudo pip3 install -i https://mirrors.sjtug.sjtu.edu.cn/pypi/web/simple pip -Usudo pip3 config set global.index-url https://mirrors.sjtug.sjtu.edu.cn/pypi/web/simple# Tsinghua tuna pypi# sudo pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U# sudo pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\n启动虚拟环境创建 homeassistant 用户并配置 python 虚拟环境用于运行程序\nsudo useradd -rm homeassistantcd /srvsudo mkdir homeassistantsudo chown homeassistant:homeassistant homeassistantsudo su -s /bin/bash homeassistantcd /srv/homeassistantpython3 -m venv .source bin/activate\n\n后期重新进入虚拟环境的命令是\nsudo -u homeassistant -H -ssource /srv/homeassistant/bin/activate\n\n可以将 sudo -u homeassistant -H -s &amp;&amp; cd ~ 写入 /home/homeassistant/.bashrc 尾部，这样当切换到 homeassistant 用户时，就会默认激活虚拟环境。\n安装并运行 Home Assistant由于程序 BUG，在安装最新版本 Home Assistant 时，不会自动下载所有依赖。经过各种资料查找和尝试，我确认了可以先安装 0.100.3 版本并运行，再安装最新版本程序。\n在控制台运行\npip3 install homeassistant==0.100.3hass\n\n等待 Home Assistant 安装所需的依赖并启动服务器。当控制台提示\n2020-06-20 18:27:32 INFO (MainThread) [homeassistant.core] Timer:starting\n\n时，代表程序已经可以正常运行了。此时 Ctrl + C 结束进程，然后安装最新版的 Home Assistant。\npip3 install homeassistant -Uhass\n\n继续等待一段时间的依赖安装，然后进入 http://IP:8123 配置 Home Assistant。\n\n\n配置 Home Assistant接入米家小米在米家 app 开放了部分绿米 IoT 设备的管理接口，允许第三方管理程序管理 IoT 智能设备。但是大多数米家设备并没有在明面上开放管理密钥。但是实际情况中，所有通过蓝牙或 Zigbee 协议连接的智能设备都需要配置网关，网关需要接入 WiFi，算上原本就通过 WiFi 接入的设备，所有米家设备都可以从内网进行访问。米家 app 也是通过内网 WiFi 进行控制的\n感谢开源社区和开发者的工作，封装好了各种插件，使我们能够直接配置获取到的 token 和设备地址，将 IoT 设备接入 Home Assistant。设备的接入方式可以在 https://www.home-assistant.io/integrations/#search/xiaomi 页面找到，配置文件样例也包含在页面中。可以通过 Android 系统下 米家 5.4.54 版本程序泄露的日志获取 token ，参考 https://zhuanlan.zhihu.com/p/62681849\n\n\n根据各处的教程，我将我的设备接入了 Home Assistant。上面的截图就是界面平时的样子，可以手动更改界面排序。我打算在之后用一块独立的电子墨水屏显示状态。由于我刚组了 Mesh，重命名了 WiFi ，所以以前的部分设备掉线了，图中没有相关的设备信息\n支持的米家设备可以在 https://www.jianguoyun.com/p/DbzdYzoQp5HMBhjZ4IkB 查询到，可以说基本覆盖全了\n桥接至 Apple HomeKit桥接至 HomeKit 的方法也很简单。在 &#x2F;home&#x2F;homeassistant&#x2F;.homeassistant 下的 configuration.yaml 中新增一行\nhomekit:\n\n然后重启程序即可。重启后就可以在左下角的通知处看到接入 HomeKit 的消息。至于自动化、联动等操作，我打算全部通过 HomeKit 和米家管理。因此，这部分配置我没有继续研究下去\n\n\nReferences\nhttps://blog.csdn.net/fanmengmeng1/article/details/46366695\nhttps://mirrors.tuna.tsinghua.edu.cn/help/raspbian/\nhttps://www.home-assistant.io/docs/installation/raspberry-pi/\nhttps://home-assistant.cc/installation/general/\nhttps://github.com/home-assistant/core/issues/28361\nhttps://www.jianguoyun.com/p/DbzdYzoQp5HMBhjZ4IkB\nhttps://zhuanlan.zhihu.com/p/62681849\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/06/run-hass-on-raspberry/ Written with Passion and Hope\n"},{"title":"我的第一台黑苹果主机","url":"//blog/2020/11/my-first-hackintosh/","content":"前言近期，拼拼凑凑从各类垃圾中组装出了一台轻便的 ITX 主机。主板是真白丢给我的，在他那不能工作的 华擎 H110M-ITX ；CPU 是我之前测试 QL3X 时点亮用的 Pentium G4500T 6 代低压 U；电源是 tb 买的韩国牌子 全汉代工 450W SFX ；机箱是 酷鱼 T40 ；内存是 影驰 镭 RGB 灯条 3200MHz 8G×2；硬盘是以前捡的 昂达 480G SSD，显卡是亮机卡 NVIDIA Quadro K600。这么一套东西装起来，我就想，能不能搞个黑苹果玩玩。于是，查了一段时间的资料，准备动手。\n\n本文仅记录点亮黑苹果的过程。其他驱动的修改预计会在后续记录。\n\n配置价格装机之前，大致估算一下目前这套机器的价格。散热我用的酷冷十几块的（AMD 原装）散热铝块，就不计入总价了。\n\n\n\n\n\n\n\n\n\n主板\nASRock H110M-ITX\n150\n\n\nCPU\nPentium G4500T\n230\n\n\n内存\n影驰 镭 RGB 灯条 3200MHz 8G×2\n400\n\n\n电源\n全汉代工 450W SFX\n130\n\n\n硬盘\n昂达 480G SSD\n240\n\n\n机箱 &amp; 延长线\n酷鱼 T40\n140\n\n\n显卡\nNVIDIA Quadro K600\n140\n\n\n总价\n\n1430\n\n\n其中内存可以不用这么好的，CPU 可以再上好一些的，显卡可以换矿卡（等矿难）。整机价格偏高，不过既然都是捡来的垃圾，那就无所谓性价比了。\n上机图片酷鱼 T40 的设计十分紧凑，附带扶手，单手就能轻松提起。机器显卡为倒装，很有意思。由于是非模组电源，走线较为凌乱。\n\n\n\n\n\n\n\n\n安装准备安装黑苹果首先需要准备一块大号U盘（至少32G）。我用了一块从 Surface 上拆下来的 PM961 + ACASIS NVMe 硬盘盒 作为安装介质。不能使用 SD 卡。一开始我使用的是 SD 卡 + 读卡器，结果一直无法通过 Etcher 写入 dmg 文件后的镜像校验。\n\n\n其次，需要准备 安装介质写入工具。我使用的是 balenaEtcher 作为写入工具。\n同时最重要的，带引导的黑苹果镜像。我主要从\n\n黑苹果星球 https://heipg.cn/\n黑果小兵 https://blog.daliansky.net/\n\n上下载镜像。\n这类镜像一般由几个分区组成。OpenCore &#x2F; Clover 引导分区，白苹果分区，WePE 分区（用于应急）。需要安装黑苹果的电脑由 OpenCore 引导启动（Clover 底层也是 OpenCore 了），模拟 Mac 相关硬件，最后启动白苹果系统。\n镜像烧录此处以从 heipg.cn 上下载的最新版本的 Catalina 为例。下载镜像，并使用 balenaEtcher 烧录进 U 盘。\n\n\n确认 Etcher 烧录没有报错后，弹出 U 盘并重新插入，准备修改 EFI。\n微调 EFI此类打包好的镜像文件一般集成了现有大多数常用驱动程序。我们首先要做的是点亮系统，再考虑其他硬件的运行情况。最开始安装黑苹果的过程中，我没有考虑 CPU 的影响，结果安装触发了 PANIC。经各种资料查阅，发现是由于我使用的奔腾 CPU 并不被苹果官方支持，需要微调 EFI，让 OC &#x2F; Clover 欺骗苹果系统。\nEFI 分区在 Windows 下不默认挂载。我们需要使用 diskpart 工具将其挂载至硬盘驱动器目录中。打开带管理员权限的 PowerShell (Admin)，执行\ndiskpart\n\n进入 diskpart 后，\nDISKPART&gt; list disk  Disk ###  Status         Size     Free     Dyn  Gpt  --------  -------------  -------  -------  ---  ---  Disk 0    Online          447 GB      0 B        *  Disk 1    Online          931 GB  1024 KB        *  Disk 2    Online          953 GB      0 B        *  Disk 4    Online          238 GB   225 GB        *\n\n选择 U 盘对应的 Disk\nDISKPART&gt; select disk 4Disk 4 is now the selected disk.DISKPART&gt; list partition  Partition ###  Type              Size     Offset  -------------  ----------------  -------  -------  Partition 1    System             200 MB    20 KB  Partition 2    Primary             12 GB   200 MB  Partition 3    Primary            380 MB    12 GB\n\n选择对应分区。EFI 分区一般为 200MB。最后进入分区并分配盘符\nDISKPART&gt; select partition 1Partition 1 is now the selected partition.DISKPART&gt; assign letter=HDiskPart successfully assigned the drive letter or mount point.\n\n运行后，EFI 分区可以在电脑中看到。修改 /EFI/OC/config.plist \n对于 Clover，修改 KernelCPU 附近的代码为\n&lt;key&gt;KernelCpu&lt;/key&gt;&lt;false/&gt;&lt;key&gt;FakeCPUID&lt;/key&gt;&lt;string&gt;0x0306A0&lt;/string&gt;\n\n对于 OpenCore，修改 Kernel - Emulate 附近的代码为\n&lt;key&gt;Cpuid1Data&lt;/key&gt;&lt;data&gt;oAYDAAAAAAAAAAAAAAAAAA==&lt;/data&gt;&lt;key&gt;Cpuid1Mask&lt;/key&gt;&lt;data&gt;/////wAAAAAAAAAAAAAAAA==&lt;/data&gt;\n\n以欺骗苹果系统，让其认为该 CPU 是 Ivy Bridge 的 i5 CPU。\n由于我使用的是 Windows 系统，需要手撸配置文件。由于 OpenCore 这项的表示方法是 id + mask，并且是 Little Endian 的。所以对于 Clover 中的 0x0306A0 ，需要将其转换为 A0 06 03 00，掩码 11 11 11 11，补齐后面 12 个 00 后，使用工具 https://cryptii.com/pipes/base64-to-hex 转换为对应的 Base 64 编码。\n太麻烦了，还是用 Clover 吧。\n设置结束后，不要忘了在 diskpart 中执行\nremove letter=H\n\n取消挂载，避免占用盘符。\nBIOS 设置BIOS 设置可以参考 https://heipg.cn/tutorial/basic-install-hackintosh-walkthrough.html 。基本思路是\n\n关闭 CFG Lock\n关闭 VT-d\n关闭核显，使用独显（在我这套硬件配置下）\n关闭 Fast Boot\n关闭 Secure Boot\n关闭 CSM\n\n实机安装插电，开机。由于我原有一个 Windows 系统，所以先进入 PE 删除原有系统。开机连点对应按键进入 Boot 菜单（我是华擎所以是 F11），第一个分区是 OpenCore，第二个是 WePE。\n这一步总是报错然后卡住。最后我用 heipg.cn 双引导 Catalina 中 Clover 引导，终于进入了系统。\n执行安装程序\n\n\n\n如果顺利，等苹果 logo 读条完成后，就可以进入 MacOS 安装界面了。\n首先选择磁盘工具，抹除原有磁盘上的所有数据。\n\n\n之后选择安装 MacOS，一路下一步即可。保持引导 U 盘插在电脑上，等待几次重启后就安装完毕了。\n\n\n建立本机引导安装完成，进入系统后，安装 Clover Configuration，然后通过 Clover Configuration 挂载隐藏的 EFI 分区，将 U 盘上的 EFI 分区下 EFI 文件夹复制到机器的 EFI 分区中。注意区分好哪个是机器的 EFI 分区，哪个是 U 盘的。\n至此，可以关机，拔掉 U 盘，重新启动等待机器自动进入 MacOS 系统了。\n\n\n\n\nReferences\nhttps://blog.daliansky.net/Updated-Frequently-Asked-Questions-in-Sierra-or-high-sierra.html\nhttps://www.mfpud.com/topics/1031/\nhttps://ark.intel.com/content/www/us/en/ark/products/90725/intel-pentium-processor-g4500t-3m-cache-3-00-ghz.html\nhttps://heipg.cn/tutorial/basic-install-hackintosh-walkthrough.html\nhttps://www.insanelymac.com/forum/topic/343348-transition-from-clover-to-opencore-with-pentium-g4560/\nhttps://oc.skk.moe/7-kernel.html\nhttps://cryptii.com/pipes/base64-to-hex\nhttps://www.bilibili.com/read/cv7941995/\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/11/my-first-hackintosh/ Written with Passion and Hope\n","categories":["Hackintosh"],"tags":["Hackintosh"]},{"title":"成为一名流量矿工","url":"//blog/2020/12/become-a-traffic-miner/","content":"\n简短地水一篇\n\n闲置了一块主板前段时间给实验室配置了软路由，为了避免意外，多买了一块主板。最近，在实验室成功部署了异地组网后，才想起来自己多买了一台机器。这台机器的配置是 1037u，我之前买过三块，分别用于各种地方的软路由和小型机。这块u价格便宜，没有 J1900 那样的溢价，性能和 J1900 差不多，功耗不高，非常适合用来做 NAS 和软路由。对于手上这块 1037u，我准备拿来挖矿。\n硬件这次的硬件没有什么好介绍的，主要如下\n\n\n\n\n\n\n\n\n主板 &#x2F; CPU\n1037u 集成主板\n\n\n内存\n杂牌 DDR3 2G\n\n\n电源\n12V DC\n\n\n系统硬盘\nminisata\n\n\n数据硬盘\nWD 500G 黑盘\n\n\n给主板来一张全貌图\n\n\n\n\n解决硬盘供电问题由于使用的是 DC 12V 供电的工控机主板，没有使用任何直插的电源板，为硬盘供电成了问题。经过仔细观察，我发现主板上存在一个 SATA_PWR 的接口，理论上可以用于硬盘的供电。\n\n\n我从万能的淘宝购入了一条转接线用于硬盘供电的转接。众所周知，硬盘主要由 5V 和 12V 供电。大多数 2.5 寸 HDD 硬盘和 SSD 硬盘只需要 5V 供电，3.5 寸 HDD 大多需要 12V 供电。这个接口提供了这两种电压，并且同时存在两种接口规格，需要测量最左侧和最右侧的两个针脚对应的电压，否则会烧坏硬盘。\n\n\n测得电压后，由于我这块主板的电压定义和店家给的线正好相反，需要将针脚中的线扯出，归位\n\n\n一定要进行上述步骤检查，否则就会像我第一次一样，烧坏了一块蓝盘（大短路，磁头烧毁）\n挖矿程序选择我选择的是星际比特 PolarOS 所属的银河计划，原理大概是运行某些互联网厂商（如 271）的边缘 CDN 项目，共享上传带宽。安装方式很简单，从官网下载 img 镜像，用烧录工具烧录至 U 盘。我用的是老朋友 balenaEtcher。烧录完成后，通过 u 盘启动机器，选择安装到系统硬盘上。运行这个挖矿程序一定要两块以上的硬盘，其中一块用来安装系统，另一块用于存储分发的数据。\n系统安装完成后，等待重启。控制台中会出现一个二维码，从微信小程序IPFS星际比特或者安卓官方 app 扫码绑定设备，并运行 银河计划。我选择银河计划的原因是因为他是现金结算项目，且每次提现仅收取 2 元手续费。\n经过三四天的运行，单台机器的收益大约是 1.5 &#x2F; 天（两台机器，30Mbps 上行）\n提高挖矿效率的一些方案由于刚开始挖矿，没有什么经验，仅根据个人猜测提供一些方案。首先一定要保持设备随时在线。前几天我调整路由器的时候进行了一次网络设备断电，直接导致了收益锐减。估计设备掉线惩罚很严重，不知道先手动停止再操作掉线会不会惩罚的少一些。其次是内存尽量保持 4G 以上，硬盘保持 500G 以上，以提供更好的挖矿能力。CPU 的选择比较随意，性能约等于 1037u，J1900 就行。建议一定要申请公网 IP ，打开光猫或者拨号设备上的 uPnP，减少外网到挖矿设备的 NAT 层数，最好直接挂在拨号设备下。\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/12/become-a-traffic-miner/ Written with Passion and Hope\n","categories":["Network","DIY","Mining"],"tags":["DIY","Mining"]},{"title":"在 Ubuntu 上安装 CUDA","url":"//blog/2020/12/install-cuda-on-ubuntu/","content":"前言老是忘记怎么在 Ubuntu 上安装 CUDA，每次有新机器都要在环境恶劣的互联网上寻找各种不靠谱又容易炸的教程。还是简单做个总结，关于如何安装炼丹人必备的依赖 CUDA。\n一些链接\nCUDA GPU Capability https://developer.nvidia.com/cuda-gpus\nCUDA Zone https://developer.nvidia.com/CUDA-zone\n\n安装流程安装前准备先看看机器的版本号啥的\nsudo lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription:    Ubuntu 18.04.2 LTSRelease:        18.04Codename:       bionic\n\n再看看显卡在不在\nlspci | grep NVIDIA18:00.0 3D controller: NVIDIA Corporation Device 1eb8 (rev a1)af:00.0 3D controller: NVIDIA Corporation Device 1eb8 (rev a1)\n\n再换个源。机器在深圳，用科大了\nsudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list\n\n安装Step 1：安装 ubuntu-drivers 并配置驱动sudo apt updatesudo apt install ubuntu-drivers-common -y\n\n运行\nubuntu-drivers devices== /sys/devices/pci0000:17/0000:17:00.0/0000:18:00.0 ==modalias : pci:balabalabalavendor   : NVIDIA Corporationdriver   : nvidia-driver-440-server - distro non-freedriver   : nvidia-driver-450-server - distro non-freedriver   : nvidia-driver-455 - distro non-free recommendeddriver   : nvidia-driver-450 - distro non-freedriver   : nvidia-driver-418-server - distro non-freedriver   : xserver-xorg-video-nouveau - distro free builtin\n\n装那个 recommended 的\nsudo apt install nvidia-driver-455\n\n\n\nStep 2：安装 CUDA去官网看看怎么下载 https://developer.nvidia.com/cuda-downloads\n可惜太慢了，可以用 阿里云 的源 https://developer.aliyun.com/mirror/nvidia-cuda\n官网选择 deb(network)，得到安装命令\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pinsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo add-apt-repository &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /&quot;sudo apt-get updatesudo apt-get -y install cuda\n\n替换下源到阿里云\nwget https://mirrors.aliyun.com/nvidia-cuda/ubuntu1804/x86_64/cuda-ubuntu1804.pinsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600sudo apt-key adv --fetch-keys https://mirrors.aliyun.com/nvidia-cuda/ubuntu1804/x86_64/7fa2af80.pubsudo add-apt-repository &quot;deb https://mirrors.aliyun.com/nvidia-cuda/ubuntu1804/x86_64/ /&quot;sudo apt-get updatesudo apt-get -y install cuda\n\n要是出现\ncuda : Depends: cuda-11-2 (&gt;= 11.2.0) but it is not going to be installed\n\n删掉 NVIDIA 相关的程序重来\nsudo apt cleansudo apt updatesudo apt purge cudasudo apt purge nvidia-*sudo apt autoremovesudo apt install cuda\n\nStep 3：测试下漫长的安装完成后，reboot ，运行 nvidia-smi 看看\nnvidia-smiThu Dec 31 12:25:41 2020+-----------------------------------------------------------------------------+| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  Tesla T4            Off  | 00000000:18:00.0 Off |                    0 || N/A   94C    P0    37W /  70W |      0MiB / 15109MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   1  Tesla T4            Off  | 00000000:AF:00.0 Off |                    0 || N/A   95C    P0    38W /  70W |      0MiB / 15109MiB |      5%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================||  No running processes found                                                 |+-----------------------------------------------------------------------------+\n\n妙啊补充感觉好像只要运行二、三步就行了\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2020/12/install-cuda-on-ubuntu/ Written with Passion and Hope\n","categories":["Linux","Docker","ML"],"tags":["NVIDIA","CUDA","Ubuntu"]},{"title":"更换软路由：T620 Plus","url":"//blog/2021/10/t620-plus-openwrt/","content":"前言拖延症是这样的，从抽屉里翻出了半年前买的、打算做软路由的 T620 Plus。这一拖就是半年，这次准备给它部署 Openwrt，替代目前使用的单口软路由。\nT620 Plus 应该被不少人推荐过，包括 B 站 UP 主司波图 https://www.bilibili.com/video/BV1J54y1m7ar。这台机器的 CPU 性能超过 J1900，且带 AES 指令集，从性能上看绰绰有余，甚至可以安装一些虚拟化应用。\n老家伙之前我使用的软路由是 1037U + 定制亚克力外壳。1037U 作为软路由来说，性能已经足够，就算用来科学上网也能够跑到 500Mbps 左右。但我使用的定制外壳防尘性能较差，处理器散热器已经有一定的积灰，且有时候会莫名卡死。且当前软路由只有一个网口，我将其以单臂路由的形式下挂在连接到光猫的交换机上，链路重复利用，可能也会导致某些不可知的问题。因此，我将其更换为 T620 Plus。\n\n\nT620 Plus我于半年前从闲鱼购入这台 T620 Plus，当时的价格为 RMB328。机器是普通小型桌面机器的大小，加装硬件部分全部采用免螺丝设计，非常好评。机器内部结构如下。\n\n\n翻开盖板后，可以看到内部结构，非常整洁。\n\n\n\n\n机器附带一个 19V、90W 电源适配器，实际用不到这么高的功耗。\n\n\n加装网卡机器本身自带一个网口，因此我又花 99 买了一张古老的 BCM5709 四口半高网卡。\n\n\n安装网卡非常容易，注意卡扣要顶住 PCIe 挡板，避免网卡移动。\n\n\n安装后的网卡不好拔网线，具体可以从司波图的视频中看到。\n性能测试我在 Win 10 下测试了机器性能。\nCPU-Z\n\n\nCPU-Z 跑分\n\n\nAIDA 64 内存和缓存测试\n\n\nAIDA 64 CPU + FPU 烤机测试，全程保持在最高 68 度，机器几乎没有噪音。烤机功耗 30W 左右，待机功耗 15-20W。\n\n\n\n\n总结总体上看，这是一台功耗低、性能还不错、噪音小的小机器，加装网卡后非常适合作为软路由使用。如果有需要，甚至可以加装万兆网卡。唯一的缺点是体积，和专门设计用于软路由场景的工控机相比，T620 Plus 还是稍大。如果有软路由需求，该机器可以纳入考虑。\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2021/10/t620-plus-openwrt/ Written with Passion and Hope\n","categories":["Network","Router","DIY"],"tags":["Router","Openwrt","Homelab"]},{"title":"Standard Notes 部署小结和使用体验","url":"//blog/2022/05/self-hosted-standard-notes/","content":"前言因为有私有记录笔记和维护版本的需求，我进行了一系列的调研，并最终选中了 Standard Notes 和 Outline 两款产品。我先尝试上手部署了 Standard Notes 并试用，前后时间约一天。在本文中，我对其部署和使用做一个简单的记录。\n近年来各类文档编辑工具类目繁多且各有特色。我最初接触的在线文档是石墨，后来随着需求和环境的变化，以及在线办公的普及，还曾使用了钉钉、企业微信等以中国大陆互联网企业为开发主体的文档工具，和 Notion 等更加国际化的文档。从使用的便捷程度和完善程度上说，我个人倾向于使用国际化的文档工具。\n但是，我使用在线文档在大多数情况下是为了和协作者进行信息共享。因此在更多的时间里，我会使用例如 Typora 这类的本地 Markdown 编辑工具，加上 Syncthing 进行多端同步以保证数据可靠。随着 Typora 的收费和 Beta 版本的完全禁用，我决定寻找一个更安全、方便的文档编辑和同步方式。\n部署在部署部分，我着重记录官网文档中提及甚少的部分，如一些功能的不可用、插件的部署。\n\n\n基本流程大致的部署流程都可以从官网或官方文档处找到。一般来说，直接 clone 官方的 main 分支仓库 git clone --single-branch --branch main https://github.com/standardnotes/standalone.git 并做好配置，即可直接通过 docker compose 启动所需的所有容器。\n在 clone 完后，执行 ./server.sh setup，程序会自动生成所需的 .env、docker/auth.env 和 docker/api-gateway.env，需要进入 .env 修改 AUTH_JWT_SECRET，同时进入 docker/auth.env 修改 JWT_SECRET、LEGACY_JWT_SECRET``PSEUDO_KEY_PARAMS_KEY 和 ENCRYPTION_SERVER_KEY。以上这些参数都可以通过 openssl rand -hex 32 生成。\n需要执行程序时，直接执行 docker compose up -d 或者使用 ./server.sh start。之后在 host 上做好反向代理，客户端就能直接连接。\n整体的部署流程是平滑且容易上手的。Standard Notes 的开发团队对开源的支持非常友好，且不断在提升 self-hosted 的部署体验。\n开启订阅部署完成后的服务端是允许任意用户注册的，但是注册完成后，用户无法使用所有的高级功能，需要我们手动将对应用户升级为订阅账户以支持更多功能。官网的文档参考 Subscriptions on your Standard Notes Standalone Server 。总结来说，需要在先前 clone 的 git 文件夹下执行以下命令。\ndocker compose exec db sh -c &#x27;MYSQL_PWD=$MYSQL_ROOT_PASSWORD mysql $MYSQL_DATABASE&#x27;\n\n然后在 MySQL Container 的 Shell 中执行，\nINSERT INTO user_roles (role_uuid , user_uuid) VALUES ( ( select uuid from roles where name=&quot;PRO_USER&quot; order by version desc limit 1 ) ,( select uuid from users where email=&quot;&lt;EMAIL@ADDR&gt;&quot; )  ) ON DUPLICATE KEY UPDATE role_uuid = VALUES(`role_uuid`);insert into user_subscriptions set uuid = UUID() , plan_name=&quot;PRO_PLAN&quot; , ends_at = 8640000000000000, created_at = 0 , updated_at = 0,user_uuid= (select uuid from users where email=&quot;&lt;EMAIL@ADDR&gt;&quot;) , subscription_id=1 , subscription_type=&#x27;regular&#x27;;\n\n扩展部署官方提供的扩展介绍在 https://docs.standardnotes.com/extensions/intro，部署文档在 Extensions Local Setup 。为了应对 self-hosted 的部署，我构建了一个 gh-page 用于存储扩展所属的静态网页 SNExts。这些扩展有的能够统计字数，有的能够支持更好的 Markdown + 富文本 编辑体验，取舍主要看个人喜好。\n软件架构经过部署和使用，我对该软件的整体架构做一个简单总结。在服务端，Standard Notes 的开发团队将软件拆分为多个微服务，以多个 Docker 部署，包括同步服务器、验证（Auth）服务器、数据库等。每个 Standard Notes 的 Client 连接到服务端时，都可以注册属于自己的用户信息（账号，密码）。账号和密码留存在服务端，客户端并不做任何账号密码的存储，因此，抛去笔记撰写客户端的外壳，本质上 Standard Notes 是一个一对多的、带加密的中心化文件同步工具。\n客户端Standard Notes 提供了跨平台的客户端，其中 Web 端和各桌面端均使用 Web 构建，甚至可以直接访问 https://app.standardnotes.com/ 以使用网页编辑器，这是非常方便的。移动端的编辑器目前较为简陋，但也在官方市场都有上架（Google Play &amp; App Store）。\n体验使用体验我试用了大约半天的时间，该软件的 UI 简洁且扁平，比较能够满足我的需求。现在的大多数流行的文本编辑软件都能做到这一点。 Standard Notes 将原本 “Folders” 插件整合至编辑器，支持以多级 Tag 形式给 Notes 打标签，对于 Note 管理来说较为方便。\nStandard Notes 使用 Workspace 的概念，在每个不同的 Workspace 中，用户能够登陆不同站点的不同账号，也能在同一个 Workspace 中使用二次密钥以使用更私有的笔记存储空间。用户的笔记是以加密后的纯文本 JSON 文件存储的，在本地存储空间中，Standard Notes 贴心地提供了一个用于离线解密的网页。用户可以通过输入密码恢复加密的、打包好的备份笔记文件。**安全性有待商榷。**当然，他们还提供了二次加密的 Private Workspace 功能 What are private workspaces?\n，但跨客户端支持存在一定问题。\n缺陷软件仍存在一定问题，特别是在 self-hosted 的场景下。Standard Notes 在 Feb.10 2022 前使用 FileSafe 插件用于图片和文件的插入，但是后续取消了该支持 Deprecation Notice: FileSafe，转而使用 Standard Notes Files Service。但是该修改暂时还没有同步到 self-hosted 的开源项目中 [Cite Github issue: Unable to start Upload Session]，但会尽快推进。这意味着目前，Standard Notes 的图片插入只能通过图床完成。\n另外，扩展（Extension）的导入并不友好，这导致我需要使用一个额外的 Web 服务器以支持更多的扩展功能。虽然目前 gh-pages 可以解决大部分的插件导入问题，但仍比直接存入用户账户来的更为繁琐。希望能够优化。\nReferences\nhttps://www.blackvoid.club/standard-notes-docker-self-hosted-alternative/\nhttps://bitsly.org/posts/standard-notes/\n\nAcknowledgements\nThank Icemic &amp; Peter for assistance.\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2022/05/self-hosted-standard-notes/ Written with Passion and Hope\n","categories":["WebApp"],"tags":["Note","WebApp"]},{"title":"卸载腾讯云安全组件","url":"//blog/2022/05/uninst-txagent/","content":"前言今天早上收到一条提醒，用来做梯子的腾讯云机器被提示封禁了。\n\n尊敬的腾讯云用户，您好！您的腾讯云账号（账号ID: *********， 昵称: ******）下的Lighthouse服务存在违规信息，涉嫌违反相关法律法规和政策，101.32.26.12已被限制访问，感谢您理解和支持！违规类型：存在通过技术手段使其成为跨境访问节点等行为违规URL：N&#x2F;A违规域名：N&#x2F;A违规标识：101.32.26.12\n\n遂怀疑是腾讯云可能存在和阿里云类似的监控用户隐私的类云盾功能，结果确实检索到了相关信息。\n处理腾讯云安全（审查）组件理论上，现在新创建的所有服务器，包括轻量都预装了这个组件。通过以下命令检查是否被插了后门。\nps -A | grep agent\n\n这时候你会看到\nroot@VM-4-9-debian:~# ps -A | grep agent   1229 ?        00:00:00 sgagent   1282 ?        00:00:00 barad_agent   1288 ?        00:00:00 barad_agent   1289 ?        00:00:00 barad_agent\n\n果然是插满了后门啊。\n解决方法很简单，参考 hostloc 论坛帖子提供的方法，直接在ssh执行\nsudo -isystemctl stop tat_agentsystemctl disable tat_agent/usr/local/qcloud/stargate/admin/uninstall.sh/usr/local/qcloud/YunJing/uninst.sh/usr/local/qcloud/monitor/barad/admin/uninstall.shrm -f /etc/systemd/system/tat_agent.servicerm -rf /usr/local/qcloudrm -rf /usr/local/sarm -rf /usr/local/agenttoolsrm -rf /usr/local/qcloudprocess=(sap100 secu-tcs-agent sgagent64 barad_agent agent agentPlugInD pvdriver )for i in $&#123;process[@]&#125;do  for A in $(ps aux | grep $i | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)  do    kill -9 $A  donedone\n\n即可。最后再使用 ps -A | grep agent 检查一次。\nReferences\nhttps://hostloc.com/thread-1004087-1-1.html\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2022/05/uninst-txagent/ Written with Passion and Hope\n","categories":["Network","VPS","Website"],"tags":["VPS","Website"]},{"title":"在 Homelab 中部署运行 Mastodon 并接入广域网","url":"//blog/2022/05/mastodon-at-homelab/","content":"前言心心念念 Mastodon 将近两年了，昨天终于打起精神，尝试自己部署一个。作为一名十年微博用户，我对社交网络的需求已经逐渐减少，但仍希望有一个干净的、能够自由交流的工具。因此，我决定部署属于自己的 Mastodon 实例。\n部署前准备据我所知，Mastodon 实例对机器性能的要求较高，特别是对内存的需求。考虑这些，决定在 Homelab 部署 Mastodon 实例。但问题依然存在，为了维护双向关系，Mastodon 要求在互联网公开，因此我使用 CloudFlare ZeroTrust Tunnel 将其公开至互联网。\n机器我选用了长期吃灰的 昂达 H310SD3-ITX 作为平台，CPU 选用 Intel Pentium G4500T 2c&#x2F;4t，内存为两条 8G DDR3 AMD 专用条。即使该平台比较老旧，但仍足够胜任一般的 Web 服务器需求。\n关于这篇文章Mastodon 的官方文档对使用纯 Docker 环境部署并不友好。因此，这篇文章主要对 Mastodon 使用 Docker Compose 部署做记录。当然也包括如何安全地将内网服务器暴露至公网。\nMastodon 部署流程经过摸索后，在集群内使用 Docker compose 起一个 Mastodon 实例仅需两个配置文件+少量部署操作。docker-compose.yml 如下，由官方 Github 仓库中获取并微调。\n由于该配置文件直接上了 production 环境，会强制 301 到 HTTPS。在测试环境中需要 Web 服务器反代并做好 SSL 配置。我在本地使用 traefik 作为 Web 服务器。\n\n\n.env.production 准备使用官方提供的 .env.production （https://github.com/mastodon/mastodon/blob/main/.env.production.sample） 文件作为模板修改。其中，LOCAL_DOMAIN 作为用户名后半段显示的域名，WEB_DOMAIN 作为 Mastodon 实例实际运行的域名。在测试部署时，可以使用 ALTERNATE_DOMAINS 允许其他临时域名。\n举例说明，假设我希望用户名显示为 @user@example.com，而站点部署在 mast.example.com ，那我就设置 LOCAL_DOMAIN=example.com，WEB_DOMAIN=example.com 即可。如有该需求，需要在 example.com 所属的 Web 服务器中部署对应跳转规则。此处样例为 Nginx。\nlocation /.well-known/webfinger &#123;  return 301 https://mast.example.com$request_uri;&#125;\n\n其次，因为使用 docker compose 部署，并将所有容器连接至 internal_network ，因此无需将数据库的端口映射至宿主机。因此，在 .env.production 中可以这样配置：\nREDIS_HOST=redisDB_HOST=dbES_HOST=es\n\n然后，默认的配置文件是将 s3 开启，es 功能关闭。其中 s3 用于存放静态文件，包括图片、头像、emoji、视频文件等，es 功能则用于全文搜索。由于我并不希望有额外的 s3 开销，寄希望于 CloudFlare CDN，因此我将 s3 关闭，并开启了 es 功能。这些功能的开关都在 .env.production 文件中显式标明。\n在配置 SECRET_KEY_BASE、OTP_SECRET、VAPID_PRIVATE_KEY 和 VAPID_PUBLIC_KEY 时，需要启动一个临时容器用于密文生成。如 docker run --rm -it tootsuite/mastodon:v3.5.3 bash 后执行 ./bin/rails secret 或 ./bin/rails mastodon:webpush:generate_vapid_key 。注：此处如果使用 .end.production 内的 rake 命令会报错。\nDocker Compose 准备下面的 docker compose 配置文件经过少量修改。截至发文，mastodon:v3.5.3 为最新 release 版本，因此我固定写入该版本。在 Mastodon Github 仓库中，默认会重新编译本地版本，在此我也注释了重新编译的部分代码。\n同时，为了后期迁移方便，我对文件结构进行了调整，全部数据放在当前目录的 data/ 文件夹下。部署中需要对文件夹权限进行微调。\nversion: &#x27;3&#x27;services:  db:    restart: always    image: postgres:14-alpine    shm_size: 256mb    networks:      - internal_network    healthcheck:      test: [&#x27;CMD&#x27;, &#x27;pg_isready&#x27;, &#x27;-U&#x27;, &#x27;postgres&#x27;]    volumes:      - ./data/postgres14:/var/lib/postgresql/data    environment:      - &#x27;POSTGRES_HOST_AUTH_METHOD=trust&#x27;  redis:    restart: always    image: redis:6-alpine    networks:      - internal_network    healthcheck:      test: [&#x27;CMD&#x27;, &#x27;redis-cli&#x27;, &#x27;ping&#x27;]    volumes:      - ./data/redis:/data  es:    restart: always    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2    environment:      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;      - &quot;cluster.name=es-mastodon&quot;      - &quot;discovery.type=single-node&quot;      - &quot;bootstrap.memory_lock=true&quot;    networks:       - internal_network    healthcheck:       test: [&quot;CMD-SHELL&quot;, &quot;curl --silent --fail localhost:9200/_cluster/health || exit 1&quot;]    volumes:       - ./data/elasticsearch:/usr/share/elasticsearch/data    ulimits:      memlock:        soft: -1        hard: -1  web:    # build: .    image: tootsuite/mastodon:v3.5.3    restart: always    env_file: .env.production    command: bash -c &quot;rm -f /mastodon/tmp/pids/server.pid; bundle exec rails s -p 3000&quot;    networks:      - external_network      - internal_network    healthcheck:      # prettier-ignore      test: [&#x27;CMD-SHELL&#x27;, &#x27;wget -q --spider --proxy=off localhost:3000/health || exit 1&#x27;]    # ports:    #   - 3000:3000    depends_on:      - db      - redis      - es    volumes:      - ./data/public/system:/mastodon/public/system  streaming:    # build: .    image: tootsuite/mastodon:v3.5.3    restart: always    env_file: .env.production    command: node ./streaming    networks:      - external_network      - internal_network      - traefik    healthcheck:      # prettier-ignore      test: [&#x27;CMD-SHELL&#x27;, &#x27;wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1&#x27;]    # ports:    #   - 4000:4000    depends_on:      - db      - redis  sidekiq:    # build: .    image: tootsuite/mastodon:v3.5.3    restart: always    env_file: .env.production    command: bundle exec sidekiq    depends_on:      - db      - redis    networks:      - external_network      - internal_network    volumes:      - ./data/public/system:/mastodon/public/system    healthcheck:      test: [&#x27;CMD-SHELL&#x27;, &quot;ps aux | grep &#x27;[s]idekiq\\ 6&#x27; || false&quot;]    Cloudflare Zero Trust Tunnel  cftunnel:    image: cloudflare/cloudflared:2022.5.3    restart: always    command: tunnel --no-autoupdate run --token YOUR_CF_TUNNEL_TOKEN    networks:      - internal_network    depends_on:      - webnetworks:  external_network:  internal_network:    internal: true\n\n环境部署我将部署环境的过程分为以下几步\n\n初始化数据库\nmigrate 数据库\n创建用户\n赋予用户 admin 权限\n设置存储位置的权限\n\n首先初始化一个临时的 postgres 容器\ndocker compose run --rm db\n\n然后开一个新终端，进入\ndocker exec -it mastodon-db-1 psql -U postgres\n\n执行创建数据库用户和数据库命令\nCREATE USER mastodon;CREATE DATABASE mastodon_production owner=mastodon;\n\n然后结束临时 postgres 容器，在当前文件夹继续执行初始化数据库命令\ndocker compose run --rm web rails db:migrate\n\n至此，就可以执行 docker compose up 启动容器，并访问主页创建用户。在创建用户完成后，进入 web 实例，将新创建的用户设为管理员\ndocker exec -it mastodon-web-1 bash# THENRAILS_ENV=production bin/tootctl accounts modify USERNAME --role admin\n\n同时记得给文件夹设置权限。\n# Storagesudo chown -R 991:991 ./data/public/system# ESsudo chown 1000:1000 ./data/elasticsearch\n\n配置 CloudFlare ZeroTrust Tunnel在 docker compose 配置文件中，我还增加了 CloudFlare 的 Tunnel 容器，用于映射至外网。首先进入 CloudFlare Zero Trust Dashboard https://dash.teams.cloudflare.com/ ，从左侧 Access - Tunnels 入口进入，新建一个 Tunnel，并将 token 拷贝至 docker compose 配置文件。\n当本地的 Tunnel Docker 容器连接上后，在页面配置对应转发域名。\n\n\n\nid\nHostname\nPath\nService\n\n\n\n1\nmast.example.com\napi&#x2F;v1&#x2F;streaming\nhttp://streaming:4000\n\n\n2\nmast.example.com\n*\nhttp://web:3000\n\n\nstreaming 容器在 api/v1/streaming 提供了一个 websocket 服务，一定注意其对应的路径优先级更高，CF 此处的优先级配置并不是最长前缀匹配。\n至此，实例已经可以正确运行并经过公网访问。\nReferences\n.env.production 文件编写 https://docs.joinmastodon.org/admin/config/#basic\nCLI 控制命令 https://docs.joinmastodon.org/admin/tootctl/#cache\nhttps://github.com/mastodon/mastodon/issues/3676\nhttps://stackoverflow.com/questions/55279515/elasticsearchexception-failed-to-bind-service-error\nhttps://pullopen.github.io/%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BA/2020/10/19/Mastodon-on-Docker.html\nhttps://docs.joinmastodon.org/\nhttps://github.com/mastodon/mastodon/issues/7612\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2022/05/mastodon-at-homelab/ Written with Passion and Hope\n","tags":["Homelab","WebApp","Mastodon"]},{"title":"保持云相册和 NAS 同步（iCloud & Google Photos）","url":"//blog/2022/06/photo-sync-from-cloud-to-nas/","content":"前言相册这种东西，堆着堆着就几万张了。粗略看了一下统计，我存放在 iCloud 上的相册中包含了 45k 张照片，在 Google Photos 上的相册有 30k 左右的照片。这些照片的成分也很复杂，包括日常拍照、屏幕截图和一堆纸片人图。因此，我需要一种方式管理这些照片。\n我的初步想法是，将照片同步至本地 NAS，使用自建工具进行管理。我本地使用的是群晖。即使是群晖这种较为成熟的 NAS 系统，也只有和 Google Drive 进行同步的工具，无法和 iCloud 进行同步。甚至在 Google Drive 同步工具中，也无法和相册进行同步。好消息是很多个人开发者开发了用于同步相册的工具。本文将记录这些工具的部署流程和使用方法。\n相册同步工具Google Photos我搜索到两个主流的、已经封装好 Docker 镜像的 Google Photos 同步工具，分别是 https://github.com/JakeWharton/docker-gphotos-sync 和 https://github.com/gilesknap/gphotos-sync。其中，前者需要使用 chromium-browser 登陆谷歌账号进行验证，后者需要通过 GCP 创建 OAuth API 进行账户验证。我本人更喜欢后者的方式，因此选择后者作为本地使用的同步工具。\n部署步骤后者的配置方式在 文档 中有详细介绍，此处做个总结。\n\n\n\n在 GCP 中创建项目：https://console.cloud.google.com/cloud-resource-manager。\n给项目 Enable Photo Library API：https://console.cloud.google.com/apis/library/photoslibrary.googleapis.com。\n前往 汉堡菜单-API-Credentials 创建OAuth 2.0 Client IDs，Type 为 Desktop：https://console.cloud.google.com/apis/credentials。中途会要求创建 OAuth consent screen，随便写，创建完成后将其 Publish。\n创建完成后，页面会允许下载一个包含了验证信息的 json 文件，修改文件名为 client_secret.json，放置在宿主机对应的 /path/to/config/ 文件夹下。\n[Docker 命令方法] 确认照片文件夹和 config 文件夹路径，编写初次运行的 Docker 命令，用于验证 Google App。docker run --rm -v /path/to/config:/config -v /path/to/storage:/storage -p 18080:18080 -it ghcr.io/gilesknap/gphotos-sync /storage --port 18080 --skip-files --skip-albums --skip-index。考虑到 8080 端口常被占用，更换了不常用端口。\n[Docker compose 方法] 准备 docker-compose.yml 文件，参考下图。执行 docker compose up 。\n将控制台输出的 https://accounts.google.com/o/oauth2/auth?xxxxx 复制到浏览器，进行登录操作。回调会默认回调至 http://localhost:18080/ ，此时需要将 localhost 修改为 NAS 的 IP 或者域名，手动触发验证回调。浏览器显示 The authentication flow has completed. You may close this window. 时，说明验证完成。此时在对应 /storage 的文件夹下会生成 .gphotos.token 文件，用于保存用户验证信息。\n修改 docker-compose.yml 或 docker 执行脚本，然后定时执行命令即可。\n\n参考 Docker Compose 文件version: &#x27;3.3&#x27;services:  gphotos-sync:    container_name: gphotos-sync    environment:      - PYTHONUNBUFFERED=1    volumes:      - ./config/:/config:rw      - /storage:/storage:rw    image: ghcr.io/gilesknap/gphotos-sync    networks:      - gphotos    ports:      - 18080:18080    command: /storage    # For OAuth authentication.    # command: /storage --port 18080 --skip-files --skip-albums --skip-indexnetworks:  gphotos:\n\niCloud PhotosiCloud 照片的下载我选择了 iCloud Photos Downloader。实际场景下我选择了第三方打包的 Docker：https://github.com/boredazfcuk/docker-icloudpd。相比于 Google Photos，iCloud Photos Downloader 的配置更为简单。直接编写 docker-compose.yml 后执行 docker compose up，配置密码和 2FA。之后执行 docker compose up -d，程序会以 synchronisation_interval 设定的间隔定期更新照片库。\nversion: &#x27;3.3&#x27;services:  icloudpd:    container_name: iCloudPD-boredazfcuk    networks:      - icloudpd    restart: always    environment:      - user=USER_NAME      - user_id=USER_ID      - group=USER_GROUP      - group_id=GROUP_ID      - apple_id=APPLE_ID      - authentication_type=2FA      - notification_type=Telegram      - telegram_token=TELEGRAM_TOKEN      - telegram_chat_id=CHAT_ID      - folder_structure=&#123;:%Y&#125;      - auto_delete=True      - notification_days=14      - synchronisation_interval=43200      - TZ=Asia/Hong_Kong    volumes:      - /path/to/config:/config      - /path/to/photos:/home/USERNAME/iCloud    image: boredazfcuk/icloudpdnetworks:  icloudpd:\n\n配置完这些自动下载工具后，下一步就是管理照片。我使用了 PhotoPrism，一个 self-hosted 的、带 AI 分类功能的图片管理工具。相关配置就是后话了。\nReferences\nhttps://github.com/gilesknap/gphotos-sync\nhttps://gilesknap.github.io/gphotos-sync/main/index.html\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2022/06/photo-sync-from-cloud-to-nas/ Written with Passion and Hope\n","tags":["NAS","iCloud","Google Photo"]},{"title":"基于 MQTT 将 OpenWRT 接入 Home Assistant","url":"//blog/2025/03/ha-mqtt-openwrt/","content":"前言惊闻米家官方接入了 Home Assistant（HA），我也是连忙重新捡起被废弃了很久的 Home Assistant。由于我的 Homelab 经过了多次升级和架构调整，已经完成了存算完全分离的架构，与之前的架构完全不同。我也会在后期重新详细介绍现有的 Homelab 架构。\n在本章中，我将着重介绍我如何通过 MQTT 和 luci-app-statistics 工具将 OpenWRT 的统计信息悉数同步至 HomeAssistant，做到如下的 Infrastructure 看板。\n\n\n\n\nHome Assistant 安装和米家接入由于 Homelab 架构的大部分程序运行于 Docker。因此，HA 的部署完全基于 Docker。这种架构保证了我运行的所有应用程序部署足够简单且极易维护。\n启动一个 HA 实例只需要以下一个 docker compose 文件\nservices:  homeassistant:    container_name: homeassistant    image: &quot;ghcr.io/home-assistant/home-assistant:stable&quot;    volumes:      - ./config:/config      - /etc/localtime:/etc/localtime:ro      - /run/dbus:/run/dbus:ro    restart: unless-stopped\n\n推荐将 /config 文件夹持久化保存且对外暴露，保证后期维护方便。\n米家集成的接入我选择通过 The Home Assistant Community Store (HACS)。这是一个社区维护的类似 HA 应用商店的东西。Docker 安装的 HA 需要进入 Docker Container 的 bash 进行手动安装。\ndocker exec -it &#x27;&lt;name of the container running homeassistant&gt;&#x27; bashwget -O - https://get.hacs.xyz | bash -\n\n安装完成后重启 HA，HACS 即生效。\n在 HACS 中搜索 Xiaomi，安装插件\n\n\n重启 HA。在 Setting - Devices 中添加 Xiaomi 插件，一路下一步，登录自己的手机账号，处理好验证接口的回调跳转，几乎所有米家设备都能无缝接入。\n接入 OpenWRT 数据HA 设计之初是作为家庭设备的管理中枢，统一管理所有 IoT 设备。路由器在某种意义上也是一个 IoT 设备，我认为一定有人造过相关的轮子，允许 HA 接收 OpenWRT 的上报数据。在理论上，想要实现这个效果，有两种方法：（1）HA 主动轮询 OpenWRT，采集数据；（2）OpenWRT 主动定时上报数据给 HA。经过我的搜索，数据较为详细、自定义程度更高的方案，是由 OpenWRT 主动上报数据给一个 Message Queuing Telemetry Transport（MQTT）服务器，HA 再主动从 MQTT 服务器上获取数据。\nMQTT 服务器安装参考，如果是 HA OS 安装方法或 HA Supervised 安装方法，能够直接从 Addon 中安装 MQTT 服务器。由于我使用的是 Container 版本的 HA，需要额外安装 MQTT 服务器，并保持 HA - MQTT 服务器 - OpenWRT 的网络联通。这在我使用的 Docker 部署方案上并不是什么难题。\nMQTT 服务器的 Docker compose 文件样例\nversion: &quot;3.7&quot;services:  # mqtt5 eclipse-mosquitto  mqtt5:    image: eclipse-mosquitto    container_name: mqtt5    ports:      - 1883:1883      - 9001:9001    volumes:      - ./config:/mosquitto/config:rw      - ./data:/mosquitto/data:rw      - ./log:/mosquitto/log:rw    restart: unless-stopped\n\n配置密码用于 HA 和 OpenWRT 读写。\ndocker exec -it mqtt5 shmosquitto_passwd -c /mosquitto/config/pwfile mqtt_openwrt # 配置 OpenWRT 访问的用户名和密码mosquitto_passwd -c /mosquitto/config/pwfile mqtt_ha # 配置 HA 访问的用户名和密码\n\nOpenWRT 配置软件包配置OpenWRT 上需要安装以下依赖，用于识别系统的状态和上报消息，包括\n\nluci-app-statistics \ncollectd-mod-mqtt\nmosquitto-client-ssl\ncollectd-mod-thermal\ncollectd-mod-uptime\ncollectd-mod-dhcpleases\ncollectd-mod-ping\ncollectd-mod-conntrack\ncollectd-mod-iwinfo\n\n直接使用 apk install 安装即可。\n小插曲\n我的 OpenWRT 还是老版本，并不支持基于 apt 的包管理。现有的 luci snapshots 镜像又基本都已经迁移到 apk 的版本了，导致我只能要么完全升级路由器，要么寻找一个仍然保持对 luci snapshots 提供支持的镜像。好消息是，我找到了。\nsrc/gz openwrt_base https://mirror.math.princeton.edu/pub/openwrt/snapshots/packages/x86_64/basesrc/gz openwrt_core  https://mirror.math.princeton.edu/pub/openwrt/snapshots/packages/x86_64/packagessrc/gz openwrt_packages https://mirror.math.princeton.edu/pub/openwrt/snapshots/packages/x86_64/packagessrc/gz openwrt_routing https://mirror.math.princeton.edu/pub/openwrt/snapshots/packages/x86_64/routingsrc/gz openwrt_telephony https://mirror.math.princeton.edu/pub/openwrt/snapshots/packages/x86_64/telephony\n\n该镜像依然保持了对 luci snapshots 的 opkg 安装方法支持，最后更新时间 2023年12月。谢谢美国人。\nMQTT 连接进入 OpenWRT 的命令行，创建或进入 /etc/collectd/conf.d 文件夹，创建文件 mqtt.conf，写入以下配置文件。\nLoadPlugin mqtt&lt;Plugin &quot;mqtt&quot;&gt;  &lt;Publish &quot;OpenWRT&quot;&gt;    Host &quot;MQTT_IP&quot;    Port &quot;1883&quot;    User &quot;mqtt_openwrt&quot;    Password &quot;设置一个密码&quot;    ClientId &quot;OpenWRT&quot;    Prefix &quot;collectd&quot;    Retain true  &lt;/Publish&gt;&lt;/Plugin&gt;\n\n设置的密码需要保持和 MQTT 服务器一致。\n配置完成后，执行\nservice collectd restart\n\n配置 HA在 HA 中，添加 MQTT 插件，并配置 IP、端口、用户名密码等。本文使用的用户名为 mqtt_ha。\n添加完成并确认能够正常连接后，进入 HA 的配置文件 configuration.yaml，添加\nmqtt: !include openwrt.yaml\n\n在同目录下新建一个 openwrt.yaml，参考这里 写入配置文件。可以手动修改下面部分的信息，用于区分设备。\ndevice:        identifiers: WRT7800        name: Router Netgear R7800        model: R7800        manufacturer: Netgear\n\n设置完成后同样重启 HA，根据系统报错微调 openwrt.yaml。如果没有问题，HA 中将出现一大堆包含统计信息的实体。\n数据美化思路善于利用 HA 中的各种 Helper，能够实现数据的美化展示。例如连接数的数据，这里提供一个思路。\n\n\n想要实现上述效果，首先需要在 HA 的 yaml 中增加信息展示。以下为一个示例。\n- name: Connections Percentage  state_topic: collectd/OpenWRT/conntrack/percent-used  unit_of_measurement: &#x27;%&#x27;  value_template: &quot;&#123;&#123; value.split(&#x27;:&#x27;)[1].split(&#x27;\\0&#x27;)[0] | int &#125;&#125;&quot;  unique_id: ap_connections_percentage  state_class: measurement  icon: &#x27;mdi:percent&#x27;  device:    identifiers: OpenWRT    name: OpenWRT    model: OpenWRT    manufacturer: Touko- name: Max Connections  state_topic: collectd/OpenWRT/conntrack/conntrack-max  unit_of_measurement: connections  value_template: &quot;&#123;&#123; value.split(&#x27;:&#x27;)[1].split(&#x27;\\0&#x27;)[0] | int &#125;&#125;&quot;  unique_id: max_ap_connections  state_class: measurement  icon: &#x27;mdi:connection&#x27;  device:    identifiers: OpenWRT    name: OpenWRT    model: OpenWRT    manufacturer: Touko\n\n然后需要创建一个 Helpers 用于展示 当前连接数 / 总连接数，类型为 Template。State template 分别为\n&#123;&#123; states(&#x27;sensor.connections&#x27;) &#125;&#125; / &#123;&#123; states(&#x27;sensor.max_connections&#x27;) &#125;&#125;\n\n\n\nUptime 同理。创建一个 Template，填入以下模板\n&#123;% set uptime = now() - states(&#x27;sensor.gnsh_main_gateway_uptime&#x27;) | as_datetime %&#125;&#123;% set hours = (uptime.total_seconds() // 3600) | int %&#125;&#123;% set minutes = ((uptime.total_seconds() % 3600) // 60) | int %&#125;&#123;% set seconds = (uptime.total_seconds() % 60) | int %&#125;&#123;&#123; hours &#125;&#125;小时 &#123;&#123; minutes &#125;&#125;分钟 &#123;&#123; seconds &#125;&#125;秒\n\n最后再手动配置 Dashboard，调整各种配色，就能实现想要的效果。\nReferences\nhttps://www.home-assistant.io/installation/linux\nhttps://hacs.xyz/\nhttps://www.home-assistant.io/integrations/\nhttps://mosquitto.org/documentation/authentication-methods/\nhttps://github.com/XiaoMi/ha_xiaomi_home\nhttps://github.com/lukdwo/OpenWRT-collectd-MQTT-HA\n\n———— License: BY-NC-SA 4.0 Link:  https://wasteland.touko.moe//blog/2025/03/ha-mqtt-openwrt/ Written with Passion and Hope\n","categories":["Network","Router"],"tags":["Router","Openwrt","Homelab","HomeAssistant","Xiaomi"]}]